

\chapter{Métodos diretos para sistemas lineares}\label{cap_sl_direto}
\thispagestyle{fancy}

Neste capítulo, discutiremos sobre métodos diretos para a resolução de sistemas lineares de $n$-equações com $n$-incógnitas. Isto é, sistemas que podem ser escritos na seguinte forma algébrica
\begin{align}
  a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n &= b_1\\
  a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n &= b_2\\
  &\vdots \\
  a_{n1}x_1 + a_{n2}x_2 + \cdots + a_{nn}x_n &= b_n.
\end{align}

\section{Eliminação gaussiana}\label{cap_sl_direto_sec_egauss}

Um sistema linear
\begin{align}
  a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n &= b_1 \label{eq:sl_fa_1}\\
  a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n &= b_2\\
  &\vdots \\
  a_{n1}x_1 + a_{n2}x_2 + \cdots + a_{nn}x_n &= b_n.\label{eq:sl_fa_n}
\end{align}
pode ser escrito na forma matricial
\begin{equation}
  A\pmb{x} = \pmb{b},
\end{equation}
onde $A = [a_{ij}]_{i,j=1}^{n,n}$ é chamada de matriz dos coeficientes, $\pmb{x}=(x_1, x_2, \dotsc, x_n)$ é o vetor (coluna) das incógnitas e $\pmb{b}=(b_1, b_2, \dotsc, b_n)$ é o vetor (coluna) dos termos constantes.

Outra forma matricial de representar o sistema \eqref{eq:sl_fa_1}-\eqref{eq:sl_fa_n} é pela chamada matriz estendida
\begin{equation}
  E = [A ~\pmb{b}].
\end{equation}
No caso, $E$ é a seguinte matriz $n \times (n+1)$
\begin{equation}
  E =
  \begin{bmatrix}
    a_{11} & a_{12} & \cdots & a_{1n} & b_1\\
    a_{21} & a_{22} & \cdots & a_{2n} & b_2\\
    \vdots & \vdots & \vdots & \vdots & \vdots\\
    a_{n1} & a_{n2} & \cdots & a_{nn} & b_n
  \end{bmatrix}
\end{equation}

O método de eliminação gaussiana consistem em realizar operações sobre as equações (sobre as linhas) do sistema \eqref{eq:sl_fa_1}-\eqref{eq:sl_fa_n} (da matriz estendida $E$) de forma a reescrevê-lo como um sistema triangular, ou diagonal. Para tanto, podemos utilizar as seguintes operações:
\begin{enumerate}[1.]
\item permutação entre as equações (linhas) ($E_i \leftrightarrow E_j$).
\item multiplicação de uma equação (linha) por um escalar não nulo ($E_i \leftarrow \lambda E_i$).
\item substituição de uma equação (linha) por ela somada com a multiplicação de uma outra por um escalar não nulo ($E_i \leftarrow E_i + \lambda E_j$).
\end{enumerate}

\begin{ex}\label{ex:egauss_exec}
  O sistema linear
  \begin{align}
    -2x_1 - 3x_2 + 2x_3 + 3x_4 &= 10\label{eq:ex_egauss_exec_sl_1}\\
    -4x_1 - 6x_2 + 6x_3 + 6x_4 &= 20\\
    -2x_1 + 4x_3 + 6x_4 &= 10\\
    4x_1 + 3x_2 - 8x_3 - 6x_4 &= -17\label{eq:ex_egauss_exec_sl_4}
  \end{align}
pode ser escrito na forma matricial $A\pmb{x}=\pmb{b}$, onde
\begin{equation}
  A =
  \begin{bmatrix}
    -2 & -3 & 2 & 3\\
    -4 & -6 & 6 & 6\\
    -2 & 0 & 4 & 6 \\
    4 & 3 & -8 & -6
  \end{bmatrix},
\end{equation}
$\pmb{x} = (x_1, x_2, x_3, x_4)$ e $\pmb{b} = (10, 20, 10, -17)$. Sua matriz estendida é
\begin{equation}
  E =
  \begin{bmatrix}
    -2 & -3 & 2 & 3 & 10\\
    -4 & -6 & 6 & 6 & 20\\
    -2 & 0 & 4 & 6 & 10\\
    4 & 3 & -8 & -6 & -17
  \end{bmatrix}
\end{equation}
Então, usando o método de eliminação gaussiana, temos
\begin{align}
  E &=
  \begin{bmatrix}
    -2 & -3 & 2 & 3 & 10\\
    -4 & -6 & 6 & 6 & 20\\
    -2 & 0 & 4 & 6 & 10\\
    4 & 3 & -8 & -6 & -17
  \end{bmatrix}
  \begin{matrix}
  \\
  E_2\leftarrow E_2 - (e_{21}/\pmb{e_{11}})E_1\\
  \\
  \\
  \end{matrix}\\
  &\sim 
  \begin{bmatrix}
    \pmb{-2} & -3 & 2 & 3 & 10\\
    0 & 0 & 2 & 0 & 0\\
    -2 & 0 & 4 & 6 & 10\\
    4 & 3 & -8 & -6 & -17
  \end{bmatrix}
  \begin{matrix}
  \\
  \\
  E_3\leftarrow E_3 - (e_{31}/\pmb{e_{11}})E_1\\
  \\
  \end{matrix}\\
  &\sim 
  \begin{bmatrix}
    \pmb{-2} & -3 & 2 & 3 & 10\\
    0 & 0 & 2 & 0 & 0\\
    0 & 3 & 2 & 3 & 0\\
    4 & 3 & -8 & -6 & -17
  \end{bmatrix}
  \begin{matrix}
  \\
  \\
  \\
  E_4\leftarrow E_4 - (e_{41}/\pmb{e_{11}})E_1\\
  \end{matrix}\\  
&\sim 
  \begin{bmatrix}
    \pmb{-2} & -3 & 2 & 3 & 10\\
    0 & 0 & 2 & 0 & 0\\
    0 & \pmb{3} & 2 & 3 & 0\\
    0 & -3 & -4 & 0 & 3
  \end{bmatrix}
  \begin{matrix}
  \\
  E_2 \leftrightarrow E_3\\
  \\
  \\
  \end{matrix}\\
&\sim 
  \begin{bmatrix}
    \pmb{-2} & -3 & 2 & 3 & 10\\
    0 & \pmb{3} & 2 & 3 & 0\\
    0 & 0 & 2 & 0 & 0\\
    0 & -3 & -4 & 0 & 3
  \end{bmatrix}
  \begin{matrix}
  \\
  \\
  \\
  E_4 \leftarrow E_4 - (e_{42}/\pmb{e_{22}})E_2\\
  \end{matrix}\\
&\sim 
  \begin{bmatrix}
    \pmb{-2} & -3 & 2 & 3 & 10\\
    0 & 3 & 2 & 3 & 0\\
    0 & 0 & \pmb{2} & 0 & 0\\
    0 & 0 & -2 & 3 & 3
  \end{bmatrix}
  \begin{matrix}
  \\
  \\
  \\
  E_4 \leftarrow E_4 - (e_{43}/\pmb{e_{33}})E_3\\
  \end{matrix}\\
    &\sim 
      \begin{bmatrix}
        \pmb{-2} & -3 & 2 & 3 & 10\\
        0 & \pmb{3} & 2 & 3 & 0\\
        0 & 0 & \pmb{2} & 0 & 0\\
        0 & 0 & 0 & \pmb{3} & 3
      \end{bmatrix}
\end{align}
Esta última matriz estendida é chamada de \emph{matriz escalonada} do sistema. Desta, temos que \eqref{eq:ex_egauss_exec_sl_1}-\eqref{eq:ex_egauss_exec_sl_4} é equivalente ao seguinte sistema triangular
\begin{align}
  -2x_1 - 3x_2 + 2x_3 + 3x_4 &= 10\\
  3x_2 + 2x_3 + 3x_4 &= 0\\
  2x_3 &= 0\\
  3x_4 &= 3.
\end{align}
Resolvendo da última equação para a primeira, temos
\begin{align}
  x_4 &= 1,\\
  x_3 &= 0,\\
  x_2 &= \frac{-2x_3 - 3x_4}{3} = -1,\\
  x_1 &= \frac{10 + 3x_2 - 3x_3 - 3x_4}{-2} = -2.
\end{align}

\ifisoctave
No \verb+GNU Octave+, podemos fazer as computações acima com o seguinte \href{https://github.com/phkonzen/notas/blob/master/src/MatematicaNumerica/cap_sl_direto/dados/ex_egauss_exec/ex_egauss_exec.m}{código}:
\verbatiminput{./cap_sl_direto/dados/ex_egauss_exec/ex_egauss_exec.m}
\fi
\end{ex}

\begin{obs}
  Para a resolução de um sistema linear $n \times n$, o método de eliminação gaussiana demanda
  \begin{equation}
    \frac{n^3}{3} + n^2 - \frac{n}{3}
  \end{equation}
multiplicações/divisões e
\begin{equation}
  \frac{n^3}{3} + \frac{n^2}{2} - \frac{5n}{6}
\end{equation}
adições/subtrações \cite{Burden2015a}.
\end{obs}

Com o mesmo custo computacional, podemos utilizar o método de eliminação gaussiana para transformar o sistema dado em um sistema diagonal.

\begin{ex}\label{ex:egauss_reduzida}
  Voltando ao exemplo anterior (Exemplo \ref{ex:egauss_exec}, vimos que a matriz estendida do sistema \eqref{eq:ex_egauss_exec_sl_1}-\eqref{eq:ex_egauss_exec_sl_4} é equivalente a
  \begin{equation}
    E \sim       
    \begin{bmatrix}
        -2 & -3 & 2 & 3 & 10\\
        0 & 3 & 2 & 3 & 0\\
        0 & 0 & 2 & 0 & 0\\
        0 & 0 & 0 & 3 & 3
      \end{bmatrix}.
  \end{equation}
Então, podemos continuar aplicando o método de eliminação gaussiana, agora de baixo para cima, até obtermos um sistema diagonal equivalente. Vejamos
\begin{align}
  E &\sim       
      \begin{bmatrix}
        -2 & -3 & 2 & 3 & 10\\
        0 & 3 & 2 & 3 & 0\\
        0 & 0 & 2 & 0 & 0\\
        0 & 0 & 0 & \pmb{3} & 3
      \end{bmatrix}
      \begin{array}{l}
      E_1 \leftarrow E_1 - (e_{14}/e_{44})E_4\\
      E_2 \leftarrow E_2 - (e_{24}/e_{44})E_4\\
      \\
      \\
    \end{array}\\
    &\sim       
      \begin{bmatrix}
        -2 & -3 & 2 & 0 & 7\\
        0 & 3 & 2 & 0 & -3\\
        0 & 0 & \pmb{2} & 0 & 0\\
        0 & 0 & 0 & 3 & 3
      \end{bmatrix}
      \begin{array}{l}
      E_1 \leftarrow E_1 - (e_{13}/e_{33})E_3\\
      E_2 \leftarrow E_2 - (e_{23}/e_{33})E_3\\
      \\
      \\
    \end{array}\\
    &\sim       
      \begin{bmatrix}
        -2 & -3 & 0 & 0 & 4\\
        0 & \pmb{3} & 0 & 0 & -3\\
        0 & 0 & 2 & 0 & 0\\
        0 & 0 & 0 & 3 & 3
      \end{bmatrix}
      \begin{array}{l}
      E_1 \leftarrow E_1 - (e_{12}/e_{22})E_2\\
      \\
      \\
      \\
    \end{array}\\
    &\sim       
      \begin{bmatrix}
        \pmb{-2} & 0 & 0 & 0 & 4\\
        0 & \pmb{3} & 0 & 0 & -3\\
        0 & 0 & \pmb{2} & 0 & 0\\
        0 & 0 & 0 & \pmb{3} & 3
      \end{bmatrix}
      \begin{array}{l}
      E_1 \leftarrow E_1/e_{11}\\
      E_2 \leftarrow E_2/e_{22}\\
      E_3 \leftarrow E_3/e_{33}\\
      E_4 \leftarrow E_4/e_{44}\\
    \end{array}\\
    &\sim       
      \begin{bmatrix}
        1 & 0 & 0 & 0 & -2\\
        0 & 1 & 0 & 0 & -1\\
        0 & 0 & 1 & 0 & 0\\
        0 & 0 & 0 & 1 & 1
      \end{bmatrix}
  \end{align}
Esta última matriz é chamada de matriz escalonada reduzida (por linhas) e a solução do sistema encontra-se em sua última coluna, i.e. $\pmb{x} = (-2, -1, 0, 1)$.

\ifisoctave
No \verb+GNU Octave+, podemos fazer as computações acima com o seguinte \href{https://github.com/phkonzen/notas/blob/master/src/MatematicaNumerica/cap_sl_direto/dados/ex_egauss_reduzida/ex_egauss_reduzida.m}{código}:
\verbatiminput{./cap_sl_direto/dados/ex_egauss_reduzida/ex_egauss_reduzida.m}
\fi
\end{ex}

\subsection*{Exercícios}

\begin{exer}\label{exer:egauss_reduzida}
  Use o método de eliminação gaussiana para obter a matriz escalonada reduzida do seguinte sistema
  \begin{align}
    -3x_1 + 2x_2 -5x_4 + x_5 &= -23\\
    -x_2 -3x_3 &= 9\\
    -2x_1 -x_2 + x_3 &= -1\\
    2x_2 - 4x_3 + 3x_4 &= 8\\
    x_1 - 3x_3 - x_5 &= 11
  \end{align}
\end{exer}
\begin{resp}
    \ifisoctave 
  \href{https://github.com/phkonzen/notas/blob/master/src/MatematicaNumerica/cap_sl_direto/dados/exer_egauss_reduzida/exer_egauss_reduzida.m}{Código.} 
  \fi
  $$
  \begin{bmatrix}
    1 & 0 & 0 & 0 & 0 & 1\\
    0 & 1 & 0 & 0 & 0 & -3\\
    0 & 0 & 1 & 0 & 0 & -2\\
    0 & 0 & 0 & 1 & 0 & 2\\
    0 & 0 & 0 & 0 & 1 & -4\\
  \end{bmatrix}
  $$
\end{resp}

\begin{exer}\label{exer:egauss_arredondamento}
  Use o método de eliminação gaussiana para obter a matriz escalonada reduzida do seguinte sistema
  \begin{align}
    -10^{-12}x_1 + 20x_2 - 3x_3 &= -1\\
    2,001x_1 + 10^{-5}x_2 - x_3 &= -2\\
    4x_1 - 2x_2 + x_3 &= 0,1
  \end{align}
\end{exer}
\begin{resp}
  \ifisoctave 
  \href{https://github.com/phkonzen/notas/blob/master/src/MatematicaNumerica/cap_sl_direto/dados/exer_egauss_arredondamento/exer_egauss_arredondamento.m}{Código.} 
  \fi
  $$
  \begin{bmatrix}
   1.0000 &  0.0000 &  0.0000 & -3.9435\E-1\\
  -0.0000 &  1.0000 & -0.0000 & -2.3179\E-1\\
   0.0000 &  0.0000 &  1.0000 &  1.2120\E+0
  \end{bmatrix}
  $$
\end{resp}

\section{Norma e número de condicionamento}\label{cap_sl_direto_sec_norma_numcond}

Nesta seção, fazemos uma rápida discussão sobre normas de vetores e matrizes, bem como do número de condicionamento de uma matriz.

\subsection{Norma $L^2$}

A norma $L^2$ de um dado vetor $v = (v_1, v_2, \dotsc, v_n) \in \mathbb{R^n}$ é definida por
\begin{equation}
  \|v\| := \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2}.
\end{equation}

\begin{prop}
  Dados os vetores $u,v \in \mathbb{R^n}$ e um escalar $\lambda\in\mathbb{R}$, temos:
  \begin{enumerate}[a)]
  \item $\|v\| \geq 0$.
  \item $\|v\| = 0 \Leftrightarrow v=0$.
  \item $\|\lambda v\| = |\lambda|\cdot \|v\|$.
  \item $\|u+v\| \leq \|u\| + \|v\|$ (desigualdade triangular).
  \item $u\cdot v \leq \|u\|\cdot\|v\|$ (desigualdade de Cauchy-Schwarz).
  \end{enumerate}
\end{prop}

\begin{ex}\label{ex:norma_vetor}
  A norma $L^2$ do vetor $v = (1, -2, 3, -4)$ é
  \begin{align}
    \|v\| &= \sqrt{v_1^2 + v_2^2 + v_3^2 + v_4^2}\\
    &= \sqrt{1^2 + (-2)^2 + 3^2 + (-4)^2}\\
    &= 5,4772.
  \end{align}

\ifisoctave
No \verb+GNU Octave+, podemos fazer as computações acima com o seguinte \href{https://github.com/phkonzen/notas/blob/master/src/MatematicaNumerica/cap_sl_direto/dados/ex_norma_vetor/ex_norma_vetor.m}{código}:
\verbatiminput{./cap_sl_direto/dados/ex_norma_vetor/ex_norma_vetor.m}
\fi
\end{ex}


A norma $L^2$ induzida de uma dada matriz real $A = [a_{ij}]_{i,j=1}^n$ é definida por
\begin{equation}
  \|A\| := \sup_{x\in\mathbb{R}^n, \|x\|=1} \|Ax\|.
\end{equation}
Pode-se mostrar que
\begin{equation}
  \|A\| = \sqrt{\lambda_{max}(A^TA)},
\end{equation}
onde $\lambda_{max}(A^TA) := \max\{|\lambda|;~\lambda\text{ é autovalor de }A^TA\}$.

\begin{prop}
  Dadas as matrizes reais $A, B$ $n\times n$, um vetor $v\in\mathbb{R}^2$ e um escalar $\lambda$, temos
  \begin{enumerate}[a)]
  \item $\|A\| \geq 0$.
  \item $\|A\|=0 \Leftrightarrow A=0$.
  \item $\|\lambda A\| = |\lambda|\cdot \|A\|$.
  \item $\|A+B\| \leq \|A\| + \|B\|$ (desigualdade triangular).
  \item $\|AB\| \leq \|A\|\|B\|$.
  \item $\|Av\| \leq \|A\|\|v\|$.
  \end{enumerate}
\end{prop}

\begin{ex}\label{ex:norma_matriz}
  A matriz
  \begin{equation}
    A =
    \begin{bmatrix}
      1 & -1 & 2\\
      -2 & \pi & 4\\
      7 & -5 & \sqrt{2}
    \end{bmatrix}
  \end{equation}
tem norma $L^2$
\begin{equation}
  \|A\| = 9,3909.
\end{equation}

\ifisoctave
No \verb+GNU Octave+, podemos fazer as computações acima com o seguinte \href{https://github.com/phkonzen/notas/blob/master/src/MatematicaNumerica/cap_sl_direto/dados/ex_norma_matriz/ex_norma_matriz.m}{código}:
\verbatiminput{./cap_sl_direto/dados/ex_norma_matriz/ex_norma_matriz.m}
\fi
\end{ex}

\subsection{Número de condicionamento}

O número de condicionamento de uma matriz é uma medida referente a propagação de erros de ocorre da sua aplicação. Mais especificamente, assumamos que seja dada uma matriz invertível $A = [a_{ij}]_{i,j=1}^{n,n}$, um vetor $x\in\mathbb{R}^n$ e uma perturbação $\delta_x\in\mathbb{R}^n$. Além disso, sejam
\begin{align}
  y &= Ax\\
  y + \delta_y &= A(x+\delta_x).
\end{align}
Ou seja, $\delta_y$ é a perturbação em $y$ propagada da aplicação de $A$ em $x$ com perturbação $\delta_x$.

Agora, podemos estimar a razão entre os erros relativos $e_{rel}(y) := \|\delta_y\|/\|y\|$ e $e_{rel}(x) := \|\delta_x\|/\|x\|$ da seguinte forma 
\begin{align}
  \frac{\frac{\|y\|}{\|\delta_y\|}}{\frac{\|x\|}{\|\delta_x\|}} &= \frac{\|y\|}{\|\delta_y\|}\frac{\|\delta_x\|}{\|x\|}\\
  &=\frac{\|Ax\|}{\|\delta_y\|}\frac{\|A^{-1}\delta_y\|}{\|x\|} \\
  &\leq \frac{\|A\|\|x\|\|A^{-1}\|\|\delta_y\|}{\|\delta_y\|\|x\|}\\
  &\leq \|A\|\|A^{-1}\|.
\end{align}
Logo, temos a seguinte estimativa de propagação de erro
\begin{equation}
  e_{rel}(y) \leq \|A\|\|A^{-1}\|e_{rel}(x).
\end{equation}

Isto nos motiva a definir o \emph{número de condicionamento} da matriz $A$ por
\begin{equation}
  \kappa(A) := \|A\|\|A^{-1}\|.
\end{equation}

\begin{obs}
  A matriz identidade tem o menor número de condicionamento, o qual é
  \begin{equation}
    \kappa(I) = 1.
  \end{equation}
\end{obs}

\begin{ex}\label{ex:kappa}
  Um exemplo de uma matriz bem condicionada é
  \begin{equation}
    A =
    \begin{bmatrix}
      1 & -1 & 2\\
      -2 & \pi & 4\\
      7 & -5 & \sqrt{2}
    \end{bmatrix},
  \end{equation}
  cujo número de condicionamento é $\kappa(A) = 13,997$.

  Já, a matriz
  \begin{equation}
    B =
    \begin{bmatrix}
      1 & 0 & 0\\
      0 & 0 & -2\\
      10^{5} & 10^{-4} & 10^{5}
    \end{bmatrix},    
  \end{equation}
  tem número de condicionamento
  \begin{equation}
    \kappa(B) = 1,5811\times 10^{14},
  \end{equation}
  o que indica que $B$ é uma matriz mal condicionada.
\end{ex}

\subsection*{Exercícios}

\begin{exer}\label{exer:norma_numcond}
  Considere o seguinte sistema linear
  \begin{align}
    10^{-12}x_1 + 20x_2 + 3x_3 &= -1,\\
    2,001x_1 + 10^{-5}x_2 + - x_3 &= -2,\\
    x_1 - 2x_2 - 0,1x_3 &= 0,1.
  \end{align}
  \begin{enumerate}[a)]
  \item Compute a norma $L^2$ do vetor dos termos constantes deste sistema.
  \item Compute a norma $L^2$ da matriz dos coeficientes deste sistema.
  \item Compute o número de condicionamento da matriz dos coeficientes deste sistema.
  \end{enumerate}
\end{exer}
\begin{resp}
  \ifisoctave 
  \href{https://github.com/phkonzen/notas/blob/master/src/MatematicaNumerica/cap_sl_direto/dados/exer_norma_numcond/exer_norma_numcond.m}{Código.} 
  \fi
  a) $2,2383$; b) $2,0323\E+1$; c) $3,5128\E+1$
\end{resp}


\section{Método de eliminação gaussiana com pivotamento parcial com escala}\label{cap_sl_direto_sec_egauss_pivo}

O método de eliminação gaussiana é suscetível a propagação dos erros de arredondamento, em particular, quando os pivôs são números próximos de zero. Isto pode ser mitigado com o chamado pivotamento parcial com escala. Nesta variação do método de eliminação gaussiana, o pivô é escolhido como sendo o candidato que é o maior em relação aos elementos em sua linha.

Dado um sistema $Ax = b$ com $n$-equações e $n$-incógnitas, o método pode ser descrito pelo seguinte pseudo-código:
\begin{enumerate}
 \item $E \leftarrow [A ~ b]$.
 \item Para $i=1, 2, \dotsc, n$, faça $s_i \leftarrow \max_{1\leq j \leq n}|e_{i,j}|$.
 \item Para $i=1, 2, \dotsc, n-1$:
   \begin{enumerate}[3.1]
   \item Compute $j$ tal que
     \begin{equation}
       \frac{e_{j,i}}{s_j} \geq \frac{e_{k,i}}{s_k},\quad\forall k=i, i+1, \dotsc, n.
     \end{equation}
     \item Permute as linhas $i$ e $j$, i.e. $E_i \leftrightarrow E_j$.
     \item Para $j=i+1, i+2, \dotsc, n$:
       \begin{enumerate}[3.3.1]
       \item $E_j \leftarrow E_j - \frac{e_{ji}}{e_{ii}}E_i$.
       \end{enumerate}
   \end{enumerate}
 \item Para $i=n, n-1, \dotsc, 2$:   
   \begin{enumerate}[4.1]
     \item Para $j=i-1, i-2, \dotsc, 1$:
       \begin{enumerate}[4.1.1]
         \item $E_j \leftarrow E_j - \frac{e_{ji}}{e_{ii}}E_i$.
       \end{enumerate}
   \end{enumerate}
\end{enumerate}

\begin{ex}\label{ex:egauss_pivo}
  Vamos empregar o método de eliminação gaussiana com pivotamento parcial com escala para resolvermos o seguinte sistema linear
  \begin{align}
    10^{-12}x_1 + 20x_2 + 3x_3 &= -1,\\
    2,001x_1 + 10^{-5}x_2 - x_3 &= -2,\\
    x_1 - 2x_2 - 0,1x_3 &= 0,1.
  \end{align}
  Para tanto, tomamos a matriz estendida
  \begin{equation}
    E =
    \begin{bmatrix}
      10^{-12} & 20 & 3 & -1\\
      2,001 & 10^{-5} & -1 & -2\\
      1 & -2 & -0,1 & 0,1
    \end{bmatrix}
  \end{equation}
  e computamos os valores máximos em módulo dos elementos de cada linha da matriz $A$, i.e.
  \begin{equation}
    s = (20,~2,001,~2).
  \end{equation}
  Agora, observamos que $e_{2,1}$ é o maior pivô em escala, pois
  \begin{equation}
    \frac{e_{11}}{s_1} = 5\times 10^{-14}, \frac{e_{21}}{s_2} = 1, \frac{e_{31}}{s_3}=0,5.
  \end{equation}
  Então, fazemos a permutação entre as linhas $1$ e $2$, de forma a obtermos
  \begin{equation}
    E =
    \begin{bmatrix}
      2,001 & 10^{-5} & -1 & -2\\
      10^{-12} & 20 & 3 & -1\\
      1 & -2 & -0,1 & 0,1
    \end{bmatrix}    
  \end{equation}
  Em seguida, eliminamos abaixo do pivô, e temos
  \begin{equation}
    E =
    \begin{bmatrix}
      2,001 & 10^{-5} & -1 & -2\\
      0 & 20 & 3 & -1\\
      0 & -2 & 3,9975\times 10^{-1} & 1,0995
    \end{bmatrix}
  \end{equation}
  Daí, o novo pivô é escolhido como $e_{22}$, pois ambos candidatos tem o mesmo valor em escala
  \begin{equation}
    \frac{e_{2,2}}{s_2} = 1, \frac{e_{3,2}}{s_3} = 1.
  \end{equation}
  Logo, eliminamos abaixo do pivô para obtermos
  \begin{equation}
    E =
    \begin{bmatrix}
      2,001 & 10^{-5} & -1 & -2\\
      0 & 20 & 3 & -1\\
      0 & 0 & 6,9975\times 10^{-1} & 9,9950
    \end{bmatrix}
  \end{equation}
  Procedendo a eliminação para cima, obtemos a matriz escalonada reduzida
  \begin{equation}
    E =
    \begin{bmatrix}
      1 & 0 & 0 & -2.8567\E-1\\
      0 & 1 & 0 & -2.6425\E-1\\
      0 & 0 & 1 & 1,4284\E+0
    \end{bmatrix}
  \end{equation}

\ifisoctave
No \verb+GNU Octave+, podemos fazer as computações acima com o seguinte \href{https://github.com/phkonzen/notas/blob/master/src/MatematicaNumerica/cap_sl_direto/dados/ex_egauss_pivo/ex_egauss_pivo.m}{código}:
\verbatiminput{./cap_sl_direto/dados/ex_egauss_pivo/ex_egauss_pivo.m}
\fi
\end{ex}

\subsection*{Exercícios}

\begin{exer}\label{exer:egauss_pivo_exec}
  Use o método de eliminação gaussiana com pivotamento parcial com escala para obter a matriz escalonada reduzida do seguinte sistema
  \begin{align}
    -2\times 10^{-12}x_1 + 10x_2 - 3\times 10^{-4}x_3 &= 2\\
    10^5x_1 + 10^{-13}x_2 - x_3 &= -2\\
    x_1 - 2x_2 + 3\times 10^{-7}x_3 &= 4
  \end{align}
\end{exer}
\begin{resp}
  \ifisoctave 
  \href{https://github.com/phkonzen/notas/blob/master/src/MatematicaNumerica/cap_sl_direto/dados/exer_egauss_pivo_exec/exer_egauss_pivo_exec.m}{Código.} 
  \fi
  $$
  \begin{bmatrix}
   1,0000 &  0,0000 &  0,0000 & 6,2588\E-1\\
   0,0000 &  1,0000 &  0,0000 & -1,6777\E+0\\
   0,0000 &  0,0000 &  1,0000 & 6,2589\E+4
  \end{bmatrix}
  $$
\end{resp}

\section{Fatoração LU}\label{cap_sl_direto_sec_lu}

A fatoração LU é uma forma eficiente de se resolver sistemas lineares. Dado um sistema $Ax = b$, a ideia é de fatorar a matriz $A$ como o produto de uma matriz triangular inferior\footnote{Do inglês, {\it low triangular matrix}.} $L$ com uma matriz triangular superior\footnote{Do inglês, {\it upper triangular matrix}.} $U$, i.e.
\begin{equation}
  A = LU.
\end{equation}
Com isso, o sistema $Ax = b$ pode ser reescrito na forma
\begin{equation}
  (LU)x = b \Leftrightarrow L(Ux) = b.
\end{equation}
Denotando, $Ux = y$, podemos resolver o seguinte sistema triangular
\begin{equation}
  Ly = b.
\end{equation}
Tendo resolvido este sistema, a solução do sistema $Ax = b$ pode, então, ser computada como a solução do seguinte sistema triangular
\begin{equation}
  Ux = y.
\end{equation}
Ou seja, a decomposição LU nos permite resolver uma sistema pela resolução de dois sistemas triangulares.

O procedimento de decomposição LU é equivalente ao método de eliminação gaussiana. Consideremos uma matriz $A = [a_{ij}]_{i,j=1}^{n,n}$, com $a_{1,1}\neq 0$. Denotando esta matriz por $U^{(1)} = [u_{i,j}^{(1)}]_{i,j=1}^{n,n} = A$ e tomando $L^{(1)} = I_{n\times n}$, observamos que a eliminação abaixo do pivô $u_{1,1}^{(1)}$, pode ser computada com as seguintes operações equivalentes por linha
\begin{equation}
  U^{(1)} = \begin{bmatrix}
    u_{1,1}^{(1)} & u_{1,2}^{(1)} & u_{1,3}^{(1)} & \ldots & u_{1,n}^{(1)} \\
    u_{2,1}^{(1)} & u_{2,2}^{(1)} & u_{2,3}^{(1)} & \ldots & u_{2,n}^{(1)} \\
    u_{3,1}^{(1)} & u_{3,2}^{(1)} & u_{3,3}^{(1)} & \ldots & u_{3,n}^{(1)} \\
    \vdots & \vdots & \vdots & \ldots & \vdots \\
    u_{n,1}^{(1)} & u_{n,2}^{(1)} & a_{n,3}^{(1)} & \ldots & u_{n,n}^{(1)}
  \end{bmatrix}
  \begin{array}{l}
    U_1^{(2)} \leftarrow U_1^{(1)}\\
    U_2^{(2)} \leftarrow U_2^{(1)} - m_{2,1}U_1^{(1)}\\
    U_3^{(2)} \leftarrow U_3^{(1)} - m_{3,1}U_1^{(1)}\\
    \vdots\\
    U_n^{(2)} \leftarrow U_n^{(1)} - m_{n,1}U_1^{(1)}
  \end{array}
\end{equation}
onde, $m_{i,1}=u_{i,1}^{(1)}/u_{1,1}^{(1)}$, $i=2, 3, \dotsc, n$.

Destas computações, obtemos uma nova matriz da forma
\begin{equation}
  U^{(2)} = \begin{bmatrix}
    u_{1,1}^{(2)} & u_{1,2}^{(2)} & u_{1,3}^{(2)} & \ldots & u_{1,n}^{(2)} \\
    0 & u_{2,2}^{(2)} & u_{2,3}^{(2)} & \ldots & u_{2,n}^{(2)} \\
    0 & u_{3,2}^{(2)} & u_{3,3}^{(2)} & \ldots & u_{3,n}^{(2)} \\
    \vdots & \vdots & \vdots & \ldots & \vdots \\
    0 & u_{n,2}^{(2)} & u_{n,3}^{(2)} & \ldots & u_{n,n}^{(2)}
  \end{bmatrix}  
\end{equation}
Observemos, também, que denotando
\begin{equation}
  L^{(2)} =
  \begin{bmatrix}
    1 & 0 & 0 & \ldots & 0\\
    m_{2,1} & 1 & 0 & \ldots & 0\\
    m_{3,1} & 0 & 1 & \ldots & 0\\
    \vdots & \vdots & \vdots & \ldots & \vdots\\
    m_{n,1} & 0 & 0 & \ldots & 1
  \end{bmatrix}
\end{equation}
temos
\begin{equation}
  A = L^{(2)}U^{(2)}.
\end{equation}
No caso de $u^{(2)}_{2,2}\neq 0$, podemos continuar com o procedimento de eliminação gaussiana com as seguintes operações equivalentes por linha
\begin{equation}
  U^{(2)} = \begin{bmatrix}
    u_{1,1}^{(2)} & u_{1,2}^{(2)} & u_{1,3}^{(2)} & \ldots & u_{1,n}^{(2)} \\
    0 & u_{2,2}^{(2)} & u_{2,3}^{(2)} & \ldots & u_{2,n}^{(2)} \\
    0 & u_{3,2}^{(2)} & u_{3,3}^{(2)} & \ldots & u_{3,n}^{(2)} \\
    \vdots & \vdots & \vdots & \ldots & \vdots \\
    0 & u_{n,2}^{(2)} & u_{n,3}^{(2)} & \ldots & u_{n,n}^{(2)}
  \end{bmatrix}  
  \begin{array}{l}
    U_1^{(3)}\leftarrow U_1^{(2)}\\
    U_2^{(3)}\leftarrow U_2^{(2)}\\
    U_3^{(3)} \leftarrow U_3^{(2)} - m_{3,2}U_2^{(2)}\\
    \vdots\\
    U_n^{(3)} \leftarrow U_n^{(2)} - m_{n,2}U_2^{(2)}\\
  \end{array}
\end{equation}
onde, $m_{i,2} = u_{i,2}^{(2)}/u_{2,2}^{(2)}$, $i=3, 4, \ldots, n$. Isto nos fornece
o que nos fornece
\begin{equation}
  U^{(3)} = \begin{bmatrix}
    u_{1,1}^{(3)} & u_{1,2}^{(3)} & u_{1,3}^{(3)} & \ldots & u_{1,n}^{(3)} \\
    0 & u_{2,2}^{(3)} & u_{2,3}^{(3)} & \ldots & u_{2,n}^{(3)} \\
    0 & 0 & u_{3,3}^{(3)} & \ldots & u_{3,n}^{(3)} \\
    \vdots & \vdots & \vdots & \ldots & \vdots \\
    0 & 0 & u_{n,3}^{(3)} & \ldots & u_{n,n}^{(3)}
  \end{bmatrix}.
\end{equation}
Além disso, denotando
\begin{equation}
  L^{(3)} =
  \begin{bmatrix}
    1 & 0 & 0 & \ldots & 0\\
    m_{2,1} & 1 & 0 & \ldots & 0\\
    m_{3,1} & m_{3,2} & 1 & \ldots & 0\\
    \vdots & \vdots & \vdots & \ldots & \vdots\\
    m_{n,1} & m_{n,2} & 0 & \ldots & 1
  \end{bmatrix}
\end{equation}
temos
\begin{equation}
  A = L^{(3)}U^{(3)}.
\end{equation}
Continuando com este procedimento, ao final de $n-1$ passos teremos obtido a decomposição
\begin{equation}
  A = LU,
\end{equation}
onde $L$ é a matriz triangular inferior
\begin{equation}
  L = L^{(n)} =   \begin{bmatrix}
    1 & 0 & 0 & \ldots & 0\\
    m_{2,1} & 1 & 0 & \ldots & 0\\
    m_{3,1} & m_{3,2} & 1 & \ldots & 0\\
    \vdots & \vdots & \vdots & \ldots & \vdots\\
    m_{n,1} & m_{n,2} & m_{n,3} & \ldots & 1
  \end{bmatrix}
\end{equation}
e $U$ é a matriz triangular superior
\begin{equation}
  U = U^{(n)} = \begin{bmatrix}
    u_{1,1}^{(n)} & u_{1,2}^{(n)} & u_{1,3}^{(n)} & \ldots & u_{1,n}^{(n)} \\
    0 & u_{2,2}^{(n)} & u_{2,3}^{(n)} & \ldots & u_{2,n}^{(n)} \\
    0 & 0 & u_{3,3}^{(n)} & \ldots & u_{3,n}^{(n)} \\
    \vdots & \vdots & \vdots & \ldots & \vdots \\
    0 & 0 & 0 & \ldots & u_{n,n}^{(n)}
  \end{bmatrix}.
\end{equation}

\begin{ex}\label{ex:lu}
  Consideremos a seguinte matriz
  \begin{equation}
    A =
    \begin{bmatrix}
      -1 & 2 & -2\\
      3 & -4 & 1\\
      1 & -5 & 3
    \end{bmatrix}.
  \end{equation}
  Então, para obtermos sua decomposição $LU$ começamos com
  \begin{equation}
    L^{(1)} =
    \begin{bmatrix}
      1 & 0 & 0\\
      0 & 1 & 0\\
      0 & 0 & 1
    \end{bmatrix}, ~
    U^{(1)} =
    \begin{bmatrix}
      -1 & 2 & -2\\
      3 & -4 & 1\\
      1 & -5 & 3
    \end{bmatrix}
  \end{equation}
  Então, observando que a eliminação abaixo do pivô $u_{1,1}=-1$ pode ser feita com as seguintes operações equivalentes por linha $U_2^{(2)} \leftarrow U_2^{(1)} - (-3)U_1^{(1)}$ e $U_3^{(2)} \leftarrow U_3^{(1)} - (-1)U_3^{(1)}$, obtemos
  \begin{equation}
    L^{(2)} =
    \begin{bmatrix}
      1 & 0 & 0\\
      -3 & 1 & 0\\
      -1 & 0 & 1
    \end{bmatrix}, ~
    U^{(1)} =
    \begin{bmatrix}
      -1 & 2 & -2\\
      0 & 2 & -5\\
      0 & -3 & 1
    \end{bmatrix}
  \end{equation}
  Agora, para eliminarmos abaixo do pivô $u_{2,2}=2$, usamos a operação $U_3^{(3)} \leftarrow U_3^{(2)} - (-3/2)U_3^{(2)}$, donde temos
  \begin{equation}
    L^{(2)} =
    \begin{bmatrix}
      1 & 0 & 0\\
      -3 & 1 & 0\\
      -1 & -1,5 & 1
    \end{bmatrix}, ~
    U^{(1)} =
    \begin{bmatrix}
      -1 & 2 & -2\\
      0 & 2 & -5\\
      0 & 0 & -6,5
    \end{bmatrix}
  \end{equation}
o que completa a decomposição tomando $L = L^{(3)}$ e $U = U^{(3)}$.

\ifisoctave
No \verb+GNU Octave+, podemos fazer as computações acima com o seguinte \href{https://github.com/phkonzen/notas/blob/master/src/MatematicaNumerica/cap_sl_direto/dados/ex_lu/ex_lu.m}{código}:
\verbatiminput{./cap_sl_direto/dados/ex_lu/ex_lu.m}
\fi
\end{ex}

\begin{ex}\label{ex:lu}
  Vamos resolver o seguinte sistema linear
  \begin{align}
    -x_1 + 2x_2 - 2x_3 &= 6\\
    3x_1 - 4x_2 + x_3 &= -11\\
    x_1 - 5x2 + 3x_3 &= -10.
  \end{align}
  No exemplo anterior (Exemplo \ref{ex:lu}), vimos que a matriz de coeficientes $A$ deste sistema admite a seguinte decomposição LU
  \begin{equation}
    \underbrace{\begin{bmatrix}
      -1 & 2 & -2\\
      3 & -4 & 1\\
      1 & -5 & 3
    \end{bmatrix}}_{A} =
  \underbrace{\begin{bmatrix}
      1 & 0 & 0\\
      -3 & 1 & 0\\
      -1 & -1,5 & 1
    \end{bmatrix}}_{L}
  \underbrace{\begin{bmatrix}
      -1 & 2 & -2\\
      0 & 2 & -5\\
      0 & 0 & -6,5
    \end{bmatrix}}_{U}
  \end{equation}
  Daí, iniciamos resolvendo o seguinte sistema triangular inferior $Ly = b$, i.e.
  \begin{align}
    &y_1 = -10 \Rightarrow y_1 = 6\\
    &-3y_1 + y_2 = -11 \Rightarrow y_2 = 7\\
    &-y_1 - 1,5y_2 + y_3 = -10 \Rightarrow y_3 = 6,5.
  \end{align}
  Por fim, computamos a solução $x$ resolvendo o sistema triangular superior $Ux=y$, i.e.
  \begin{align}
    &-6,5x_3 = 6,5 \Rightarrow x_3 = -1,\\
    &2x_2 - 5x_3 = 7 \Rightarrow x_2 = 1\\
    &-x_1 + 2x_2 - 2x_3 = 6 \Rightarrow x_1 = -2.
  \end{align}

\ifisoctave
No \verb+GNU Octave+, podemos fazer as computações acima com o seguinte \href{https://github.com/phkonzen/notas/blob/master/src/MatematicaNumerica/cap_sl_direto/dados/ex_lu/ex_lu.m}{código}:
\verbatiminput{./cap_sl_direto/dados/ex_lu_sol/ex_lu_sol.m}
\fi
\end{ex}

\subsection{Fatoração LU com pivotamento parcial}

O algoritmo discutido acima não prevê a necessidade de permutações de linhas no processo de eliminação gaussiana. Isto pode ser corrigido com a utilização de matrizes de permutação.

Assim como na eliminação gaussiana com pivotamento parcial, na fatoração LU com pivotamento parcial o pivô fazemos permutações de linha na matriz de forma que o pivô seja sempre aquele de maior valor em módulo. Por exemplo, suponha que o elemento $a_{3,1}$ seja o maior valor em módulo na primeira coluna da matriz $A = U^{(1)}$ com
\begin{equation}
  U^{(1)} = \begin{bmatrix}
    u_{1,1}^{(1)} & u_{1,2}^{(1)} & u_{1,3}^{(1)} & \ldots & u_{1,n}^{(1)} \\
    u_{2,1}^{(1)} & u_{2,2}^{(1)} & u_{2,3}^{(1)} & \ldots & u_{2,n}^{(1)} \\
    \pmb{u_{3,1}^{(1)}} & u_{3,2}^{(1)} & u_{3,3}^{(1)} & \ldots & u_{3,n}^{(1)} \\
    \vdots & \vdots & \vdots & \ldots & \vdots \\
    u_{n,1}^{(1)} & u_{n,2}^{(1)} & a_{n,3}^{(1)} & \ldots & u_{n,n}^{(1)}
  \end{bmatrix}.
\end{equation}
Neste caso, o procedimento de eliminação na primeira coluna deve usar $u_{3,1}^{(1)}$ como pivô, o que requer a permutação entre as linhas $1$ e $3$ ($U_1^{(1)} \leftrightarrow U_3^{(1)}$). Isto pode ser feito utilizando-se da seguinte matriz de permutação
\begin{equation}
  P =
  \begin{bmatrix}
    0 & 0 & 1 & \ldots & 0\\
    0 & 1 & 0 & \ldots & 0\\
    1 & 0 & 0 & \ldots & 0\\
    \vdots & \vdots & \vdots & \ldots & \vdots\\
    0 & 0 & 0 & \ldots & 1
  \end{bmatrix}.
\end{equation}
Com essa, iniciamos o procedimento de decomposição LU com $PA = L^{(1)}U^{(1)}$, onde $L^{(1)} = I_{n\times n}$ e $U^{(1)} = PA$. Caso sejam necessárias outras mudanças de linhas no decorrer do procedimento de decomposição, a matriz de permutação $P$ deve ser atualizada apropriadamente.

\begin{ex}
  Vamos fazer a decomposição LU com pivotamento parcial da seguinte matriz
  \begin{equation}
    A = \begin{bmatrix}
      -1 & 2 & -2\\
      3 & -4 & 1\\
      1 & -5 & 3
    \end{bmatrix}
  \end{equation}
  Começamos, tomando
  \begin{equation}
    P^{(1)} =
    \begin{bmatrix}
      1 & 0 & 0\\
      0 & 1 & 0\\
      0 & 0 & 1
    \end{bmatrix},\quad
    L^{(1)} =
    \begin{bmatrix}
      1 & 0 & 0\\
      0 & 1 & 0\\
      0 & 0 & 1      
    \end{bmatrix},\quad
    U^{(1)} = 
    \begin{bmatrix}
      -1 & 2 & -2\\
      3 & -4 & 1\\
      1 & -5 & 3
    \end{bmatrix}
  \end{equation}
  O candidato a pivô é o elemento $u_{2,1}$. Então, fazemos as permutações de linhas $P_1\leftrightarrow P_2$ e $U_1\leftrightarrow U_2$ e, na sequência, as operações elementares por linhas $U_{2:3}\leftarrow U_{2:3}-m_{2:3,1}U_1$, donde obtemos
  \begin{align}
    P^{(2)} &=
    \begin{bmatrix}
      0 & 1 & 0\\
      1 & 0 & 0\\
      0 & 0 & 1
    \end{bmatrix},\\
    L^{(2)} &=
    \begin{bmatrix}
      1 & 0 & 0\\
      -0,\overline{3} & 1 & 0\\
      0,\overline{3} & 0 & 1      
    \end{bmatrix},
    U^{(2)} = 
    \begin{bmatrix}
      3 & -4 & 1\\
      0 & 0,\overline{6} & -1,\overline{6}\\
      0 & -3,\overline{6} & 2,\overline{6}
    \end{bmatrix}
  \end{align}
  Agora, o candidato a pivô é o elemento $u_{3,2}$. Assim, fazemos as permutações de linhas $P_2\leftrightarrow P_3$, $U_2 \leftrightarrow U_3$ (análogo para os elementos da coluna 1 de $L$) e, então, a operação elementar por linha $U_3\leftarrow U_3 - m_{3,2}U_2$. Com isso, obtemos
  \begin{align}
    P^{(2)} &=
    \begin{bmatrix}
      0 & 1 & 0\\
      0 & 0 & 1\\
      1 & 0 & 0
    \end{bmatrix},\\
    L^{(2)} &=
    \begin{bmatrix}
      1 & 0 & 0\\
      0,\overline{3} & 1 & 0\\
      -0,\overline{3} & -0,\overline{18} & 1      
    \end{bmatrix},
    U^{(2)} = 
    \begin{bmatrix}
      3 & -4 & 1\\
      0 & -3,\overline{6} & 2,\overline{6}\\
      0 & 0 & -1,\overline{18}
    \end{bmatrix}
  \end{align}
  Com isso, temos obtido a decomposição LU de $A$ na forma
  \begin{equation}
    PA = LU,
  \end{equation}
  com $P=P^{(3)}$, $L=L^{(3)}$ e $U=U^{(3)}$.

\ifisoctave
No \verb+GNU Octave+, podemos fazer as computações acima com o seguinte \href{https://github.com/phkonzen/notas/blob/master/src/MatematicaNumerica/cap_sl_direto/dados/ex_lup/ex_lup.m}{código}:
\verbatiminput{./cap_sl_direto/dados/ex_lup/ex_lup.m}
\fi
\end{ex}

\subsection*{Exercícios}

\begin{exer}\label{exer:lup_sol}
  Resolva o sistema
  \begin{align}
    -x_1 + 2x_2 - 2x_3 &= -1\\
    3x_1 - 4x_2 + x_3 &= -4\\
    -4x_1 - 5x_2 + 3x_3 &= 20
  \end{align}
  usando fatoração LU com pivotamento parcial.
\end{exer}
\begin{resp}
  \ifisoctave 
  \href{https://github.com/phkonzen/notas/blob/master/src/MatematicaNumerica/cap_sl_direto/dados/exer_lup_sol/exer_lup_sol.m}{Código.} 
  \fi
  $x_1 = -3$, $x_2=-1$, $x_3 = 1$
\end{resp}
