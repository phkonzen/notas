%Este trabalho está licenciado sob a Licença Atribuição-CompartilhaIgual 4.0 Internacional Creative Commons. Para visualizar uma cópia desta licença, visite http://creativecommons.org/licenses/by-sa/4.0/deed.pt_BR ou mande uma carta para Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.

\chapter{Sistemas Não Lineares e Otimização}\label{cap_otimizacao}

Neste capítulo, apresentam-se métodos numéricos para a resolução de sistemas não lineares e de problemas de otimização. Salvo explicitado ao contrário, assume-se que os problemas são bem definidos.

\section{Sistemas Não-Lineares}\label{cap_otimizacao_sec_sisnlin}

Consideramos o seguinte problema: \hl{dada a função $F:D\subset \mathbb{R}^n\to\mathbb{R}^n$ encontrar $\pmb{x}^*\in\mathbb{R}^n$ tal que}
\begin{equation}\hleq
  F(\pmb{x}^*) = 0.
\end{equation}
Salvo explicitado ao contrário, assumiremos que $F\in C^1(D)$, i.e. $F$ é uma função continuamente diferenciável no domínio computacional $D\subset\mathbb{R}^n$. 

Vamos, denotar por $J_F(\pmb{x}) = [\jmath_{i,j}(\pmb{x})]_{i,j=1}^{n,n}$ a \hlemph{matriz jacobiana}{\jacobi} da F com
\begin{equation}\hleq
  \jmath_{i,j}(\pmb{x}) = \frac{\p f_i(\pmb{x})}{\p x_j},
\end{equation}
onde $F(\pmb{x}) = (f_1(\pmb{x}), f_2(\pmb{x}), \dotsc, f_n(\pmb{x}))$ e $\pmb{x} = (x_1, x_2,\dotsc, x_n)$.

\subsection{Método de Newton}\label{cap_otimizacao_sec_sisnlin_ssec_newton}

A iteração básica do \hlemph{método de Newton}{\newton} para sistemas de equações consiste em: dada uma aproximação inicial $\pmb{x}^{(0)}\in\mathbb{R}^n$,
\begin{align}
  &\text{resolver}\quad \hleq J_F\left(\pmb{x}^{(k)}\right)\pmb{\delta}^{(k)} = -F\left(\pmb{x}^{(k)}\right)\\
  &\text{computar}\quad \hleq \pmb{x}^{(k+1)} = \pmb{x}^{(k)} + \pmb{\delta}^{(k)}
\end{align}
para $k = 0, 1, 2, \ldots$ até que um critério de parada seja satisfeito.

\begin{obs}[\hl{Convergência}]\label{cap_otimizacao_sec_newton:obs:convNewton}
  Para $\pmb{x}^{(0)}$ suficientemente próximo da solução $\pmb{x}^*$, \hl{o método de Newton é quadraticamente convergente}. Mais precisamente, este resultado de convergência local requer que $J_F^{-1}(\pmb{x}^*)$ seja não singular e que $J_F$ seja Lipschitz{\lipschitz} contínua. Consulte \cite[Seção 7.1]{Quarteroni2007a} para mais detalhes.
\end{obs}

\begin{ex}\label{ex:burgers}
  Consideramos a \emph{equação de Burgers}{\burgers}
  \begin{equation}
    \frac{\p u}{\p t} + u\frac{\p u}{\p x} = \nu\frac{\p^2 u}{\p x^2}
  \end{equation}
  com $\nu>0$, condição inicial
  \begin{equation}
    u(0,x) = \sin(\pi x)
  \end{equation}
  e condições de contorno de Dirichlet{\dirichlet} homogêneas
  \begin{equation}
    u(t,0) = u(t,1) = 0.
  \end{equation}

  Aplicando o \emph{método de Rothe}{\rothe} com aproximação de Euler{\euler} implícita, obtemos
  \begin{equation}
    \begin{aligned}
      & \frac{u(t+h_t,x) - u(t,x)}{h_t} +\\
      &\quad u(t+h_t,x)u_x(t+h_t,x) \approx \nu u_{xx}(t+h_t,x),
    \end{aligned}
  \end{equation}
  onde $h_t>0$ é o passo no tempo. Agora, aplicamos diferenças finitas para obter
  \begin{equation}
    \begin{aligned}
      &\frac{u(t+h_t,x_i) - u(t,x_i)}{h_t} \\
      &\quad + u(t+h_t, x_i)\frac{u(t+h_t,x_{i+1})-u(t+h_t,x_i)}{h_x} \\
      &\quad \approx \nu\frac{u(t+h_t,x_{i-1}) - 2u(t+h_t,x_i) + u(t+h_t,x_{i+1})}{h_x^2},
    \end{aligned} 
 \end{equation}
  onde, $x_i=(i-1)h_x$, $i=1,\dotsc,n_x$ e $h_x=1/(n_x-1)$ é o tamanho da malha.

  Rearranjando os termos e denotando $w^{(k)}_i\approx u(t_k, x_i)$, $t_k = (k-1)h$, obtemos o seguinte \emph{problema discreto}
  \begin{align}
    & w^{(k+1)}_1 = 0\\
    & \frac{1}{h_t}w^{(k+1)}_i - \frac{1}{h_t}w^{(k)}_i + \frac{1}{h_x}w^{(k+1)}_iw^{(k+1)}_{i+1} \nonumber\\
    &\quad - \frac{1}{h_x}w^{(k+1)}_iw^{(k+1)}_i - \frac{\nu}{h_x^2}w^{(k+1)}_{i-1} \nonumber\\ 
    &\quad + 2\frac{\nu}{h_x^2}w^{(k+1)}_i - \frac{\nu}{h_x^2}w^{(k+1)}_{i+1} = 0,\\
    & w^{(k+1)}_{n_x} = 0,
  \end{align}
  sendo $w^{(0)}_i = \sen(\pi x_i)$, $i = 1, \dotsc, n_x$ e $k = 1, 2, \ldots$.

  Este problema pode ser reescrito como segue: para cada $k = 1, 2, \ldots$, encontrar $\pmb{v}\in\mathbb{R}^{n_x}$, tal que
  \begin{equation}
    F\left(\pmb{v}; \pmb{v}^{(0)}\right) = 0, 
  \end{equation}
  onde $v_{i} \approx v^{(k+1)}_i$, $v_{i}^{(0)} \approx v^{(k)}_{i}$ e
  \begin{align}
    & f_1\left(\pmb{v}; \pmb{v}^{(0)}\right) = v_1,\\
    & f_{i}\left(\pmb{v}; \pmb{v}^{(0)}\right) = \frac{1}{h_t}v_i - \frac{1}{h_t}v^{(0)}_i + \frac{1}{h_x}v_iv_{i+1} \nonumber\\
    &\quad - \frac{1}{h_x}v_iv_i - \frac{\nu}{h_x^2}v_{i-1} + 2\frac{\nu}{h_x^2}v_i - \frac{\nu}{h_x^2}v_{i+1},\\
    &f_{n_x}\left(\pmb{v}; \pmb{v}^{(0)}\right) = v_{n_x}.
  \end{align}
  A matriz jacobiana associada $J=[\jmath_{i,j}]_{i,j}^{n_x,n_x}$ contém
  \begin{align}
    & \jmath_{i,j} = 0,\quad j\neq i-1,i,i+1,\\
    & \jmath_{1,1} = 1,\\
    & \jmath_{1,2} = 0,\\
    & \jmath_{i,i-1} = -\frac{\nu}{h_x^2},\\
    & \jmath_{i,i} = \frac{1}{h_t} + \frac{1}{h_x}v_{i+1} - \frac{2}{h_x}v_i + \frac{2\nu}{h_x^2},\\
    & \jmath_{i,i+1} = \frac{1}{h_x}v_{i} - \frac{\nu}{h_x^2},\\
    & \jmath_{n_x,n_x-1} = 0\\
    & \jmath_{n_x,n_x} = 1.
  \end{align}

  Implemente este esquema numérico!
\end{ex}

\begin{exer}\label{exer:burgers}
  Considere o problema discreto apresentado no Exemplo \ref{ex:burgers} para diferentes valores do coeficiente de difusão $\nu=1., 0.5, 0.1, 0.01, 0.001$. Simule o problema com cada uma das seguintes estratégias e as compare quanto ao desempenho computacional.
  \begin{enumerate}[a)]
  \item Simule-o aplicando o Método de Newton com o {\it solver} linear \lstinline+npla.solve+.
  \item Observe que a jacobiana é uma matriz tridiagonal. Simule o problema aplicando o Método de Newton com o {\it solver} linear \lstinline+npla.solve_banded+.
  \item Aloque a jacobiana como uma matriz esparsa. Então, simule o problema aplicando o Método de Newton com {\it solver} linear adequado para matrizes esparsas.
  \end{enumerate}
\end{exer}

\section{Método Tipo Newton}\label{cap_otimizacao_sec_tipoNewton}

Existem várias modificações do Método de Newton\footnote{Isaac Newton, 1642 - 1727, matemático, físico, astrônomo, teólogo e autor inglês. Fonte: \href{https://pt.wikipedia.org/wiki/Isaac_Newton}{Wikipédia}.} que buscam reduzir o custo computacional. Há estratégias para simplificar as computações da matriz jacobiana\footnote{Carl Gustav Jakob Jacobi, 1804 - 1851, matemático alemão. Fonte: \href{https://pt.wikipedia.org/wiki/Carl_Gustav_Jakob_Jacobi}{Wikipédia}.} e para reduzir o custo nas computações das atualizações de Newton.

\subsection{Atualização Cíclica da Matriz Jacobiana}

Geralmente, ao simplificarmos a matriz jacobina $J_F$ ou aproximarmos a atualização de Newton $\delta^{(k)}$, perdemos a convergência quadrática do método (consulte a Observação \ref{obs:convNewton}). A ideia é, então, buscar uma convergência pelo menos super-linear, i.e.
\begin{equation}
  \|e^{(k+1)}\|\approx \rho_k\|e^{(k)}\|,
\end{equation}
com $\rho_k\to 0$ quando $k\to\infty$. Aqui, $e^{(k)}:=x^*-x^{(k)}$. Se a convergência é superlinear, então podemos usar a seguinte aproximação
\begin{equation}
  \rho_k \approx \frac{\|\delta^{(k)}\|}{\|\delta^{(k-1)}\|}
\end{equation}
ou, equivalentemente,
\begin{equation}
  \rho_k \approx \left(\frac{\|\delta^{(k)}\|}{\|\delta^{(0)}\|}\right)^{\frac{1}{k}}
\end{equation}
Isso mostra que podemos acompanhar a convergência das iterações pelo fator
\begin{equation}
  \beta_k = \frac{\|\delta^{(k)}\|}{\|\delta^{(0)}\|}.
\end{equation}
Ao garantirmos $0<\beta_k<1$, deveremos ter uma convergência superlinear.

Vamos, então, propor o seguinte  método tipo Newton de atualização cíclica da matriz jacobiana.
\begin{enumerate}
\item Escolha $0<\beta<1$
\item $J := J_F(x^{(0)})$
\item $J\delta^{(0)}=-F(x^{(0)})$
\item $x^{(1)} = x^{(0)} + \delta^{(0)}$
\item Para $k=1,\ldots$ até critério de convergência:
  \begin{enumerate}
  \item $J\delta^{(k)}=-F(x^{(k)})$
  \item $x^{(k+1)}=x^{(k)} + \delta^{(k)}$
  \item Se $\|\delta^{(k)}\|/\|\delta^{(0)}\| > \beta$:
    \begin{enumerate}
    \item $J := J_F(x^{(k)})$
    \end{enumerate}
  \end{enumerate}
\end{enumerate}

\begin{exer}
  Implemente uma versão do Método Tipo Newton apresentado acima e aplique-o para simular o problema discutido no Exemplo \ref{ex:burgers} para $\nu = 1., 0.1, 0.01, 0.001, 0.0001$. Faça uma implementação com suporte para matrizes esparsas.
\end{exer}

\section{Problemas de Minimização}\label{cap_otimizacao_sec_minimi}

Vamos considerar o seguinte \emph{problema de minimização}: dada a \emph{função objetivo} $f:D\subset \mathbb{R}^n\to\mathbb{R}$, resolver
\begin{equation}
  \min_{x\in D} f(x).
\end{equation}
No que segue e salvo dito explicitamente ao contrário, vamos assumir que o problema está bem determinado e que $f$ é suficientemente suave. Ainda, vamos assumir as seguintes notações:
\begin{itemize}
\item gradiente de $f$
  \begin{equation}
    \nabla f(x) = \left(\frac{\p f}{\p x_1}(x),\dotsc,\frac{\p f}{\p x_n}(x)\right)
  \end{equation}
\item derivada direcional de $f$ com respeito a $d\in\mathbb{R}^n$
  \begin{equation}
    \frac{\p f}{\p d} = \nabla f(x)\cdot d
  \end{equation}
\item matriz hessiana de f, $H=[h_{i,j}]_{i,j=1}^{n,n}$
  \begin{equation}
    h_{i,j}(x) = \frac{\p^2 f}{\p x_i\p x_j}
  \end{equation}
\end{itemize}

\begin{obs}[Condições de Otimização]
  Se $\nabla f(x^*) = 0$ e $H(x^*)$ é positiva definida, então $x^*$ é um mínimo local de $f$ em uma vizinhança não vazia de $x^*$. Consulte mais em \cite[Seção 7.2]{Quarteroni2007a}. Um ponto $x^*$ tal que $\nabla f(x^*) = 0$ é chamado de \emph{ponto crítico}.
\end{obs}

\subsection{Métodos de Declive}

Um método de declive consiste em uma iteração tal que: dada uma aproximação inicial $x^{(0)}\in\mathbb{R}^n$, computa-se
\begin{equation}
  x^{(k+1)} = x^{(k)} + \alpha_k d^{(k)},
\end{equation}
com \emph{tamanho de passo} $\alpha_k>0$, para $k=0,1,2,\ldots$ até convergência. As \emph{direções descendentes} são tais que
\begin{align}
  &d^{(k)}\cdot\nabla f(x^{(k)}) < 0,\quad \text{se } \nabla f(x^{(k)}) \neq 0,\label{eq:condDirecoes0}\\
  &d^{(k)} = 0,\quad \text{noutro caso.}\label{eq:condDirecoes1}
\end{align}

\begin{obs}(Condição de Convergência)
  Da Série de Taylor\footnote{Brook Taylor, 1685 - 1731, matemático britânico. Fonte: \href{https://pt.wikipedia.org/wiki/Brook_Taylor}{Wikipédia}.}, temos que
  \begin{equation}
    f(x^{(k)}+\alpha_kd^{(k)}) - f(x^{(k)}) = \alpha_k\nabla f(x^{(k)})\cdot d^{(k)} + \varepsilon,
  \end{equation}
  com $\varepsilon\to 0$ quando $\alpha_k\to 0$. Como consequência da continuidade da $f$, para $\alpha_k$ suficientemente pequeno, o sinal do lado esquerdo é igual a do direito desta última equação. Logo, para tais $\alpha_k$ e $d^{(k)}\neq 0$ uma direção descendente temos garantido que
  \begin{equation}
    f(x^{(k)}+\alpha_kd^{(k)}) < f(x^{(k)}).
  \end{equation}
\end{obs}


Notamos que um método de declive fica determinado pelas escolhas da direção de declive $d_k$ e o tamanho do passo $\alpha_k$. Primeiramente, vamos a este último item.

\subsubsection{Pesquisa linear}

A computação de $\alpha_k$ consiste em resolver o seguinte problema de minimização:
\begin{equation}
  \min_{\alpha\in\mathbb{R}} \phi(\alpha) = f(x^{(k)}+\alpha d^{(k)}).
\end{equation}
Entretanto, a resolução exata deste problema é muitas vezes não factível. Técnicas de aproximações para a resolução deste problema são, então, aplicadas. Tais técnicas são chamadas de \emph{pesquisa linear não exata}.

A \emph{condição de Armijo} é que a escolha de $\alpha_k$ deve ser tal que
\begin{equation} \label{eq:condArmijo}
  f\left(x^{(k)}+\alpha_k d^{(k)}\right) \leq f(x^{(k)}) + \sigma \alpha_k \nabla f\left(x^{(k)})\cdot d^{(k)}\right),
\end{equation}
para alguma constante $\sigma\in (0, 1/2)$. Ou seja, a redução em $f$ é esperada ser proporcional à derivada direcional de $f$ com relação a direção $d^{(k)}$ no ponto $x^{(k)}$. Em aplicações computacionais, $\sigma$ é normalmente escolhido no intervalo $[10^{-5}, 10^{-1}]$. 

A condição \eqref{eq:condArmijo} não é suficiente para evitar escolhas muito pequenas de $\alpha_k$. Para tanto, pode-se empregar a \emph{condição de curvatura}, a qual requer que
\begin{equation}\label{eq:condCurvatura}
  \nabla f\left(x^{(k)} + \alpha_kd^{(k)}\right)\cdot d^{(k)} \geq \beta\nabla f\left(x^{(k)})d^{(k)}\right),
\end{equation}
para $\beta\in [\sigma, 1/2]$. Notemos que o lado esquerdo de \eqref{eq:condCurvatura} é igual a $\phi'(\alpha_k)$. Ou seja, este condição impõe que $\alpha_k$ seja maior que $\beta\phi'(0)$. Normalmente, escolhe-se $\beta\in [10^{-1}, 1/2]$. Juntas, \eqref{eq:condArmijo} e \eqref{eq:condCurvatura} são conhecidas como \emph{condições de Wolfe}\footnote{Philip Wolfe, 1927 - 2016, matemático estadunidense. Fonte: \href{https://pt.wikipedia.org/wiki/Philip_Wolfe}{Wikipédia}.}.

\subsection{Método do Gradiente}

O Método do Gradiente (\emph{Método do Máximo Declive}) é um Método de Declive tal que as direções descendentes são o oposto do gradiente da $f$, i.e.
\begin{equation}
  d^{(k)} = -\nabla f(x^{(k)}).
\end{equation}
É fácil verificar que as condições \eqref{eq:condDirecoes0}-\eqref{eq:condDirecoes1} são satisfeitas.

\begin{ex}\label{ex:Rosenbrock}
  Consideremos o problema de encontrar o mínimo da \emph{função de Rosenbrock}\footnote{Howard Harry Rosenbrock, 1920 - 2010, engenheiro britânico. Fonte: \href{https://en.wikipedia.org/wiki/Howard_Harry_Rosenbrock}{Wikipedia}.}
  \begin{equation}
    f(x) = \sum_{i=1}^n 100\left(x_{i+1}-x_i^2\right)^2 + (1-x_i)^2.
  \end{equation}
  O valor mínimo desta função é zero e ocorre no ponto $x=1$. Esta função é comumente usada como caso padrão para teste de métodos de otimização.

  Para o Método do Gradiente, precisamos de suas derivadas parciais:
  \begin{gather}
    \frac{\p f}{\p x_1} = -400x_1\left(x_2-x_1^2\right) - 2(1-x_1)\\
    ~\nonumber\\
    \frac{\p f}{\p x_j} = \sum_{i=1}^n 200\left(x_i-x_{i-1}^2\right)(\delta_{i,j}-2x_{i-1}\delta_{i-1,j})-2(1-x_{i-1})\delta_{i-1,j}\\
    ~\nonumber\\
    \frac{\p f}{\p x_{n}} = 200\left(x_n - x_{n-1}^2\right)
  \end{gather}

  % \lstinputlisting[caption=Algoritmo do Gradiente, label={lst:algOtimMG}]{./cap_otimizacao/dados/pyMG/main.py}

  \begin{lstlisting}[caption=Algoritmo do Gradiente, label={lst:algOtimMG}]
import numpy as np
import numpy.linalg as npla
import scipy.optimize as spopt

# fun obj
  def fun(x):
  '''
  Função de Rosenbrock
  '''
  return sum(100.*(x[1:]-x[:-1]**2.)**2. + (1.-x[:-1])**2.)

# gradiente da fun
def grad(x):
  xm = x[1:-1]
  xm_m1 = x[:-2]
  xm_p1 = x[2:]
  der = np.zeros_like(x)
  der[1:-1] = 200*(xm-xm_m1**2) - 400*(xm_p1 - xm**2)*xm - 2*(1-xm)
  der[0] = -400*x[0]*(x[1]-x[0]**2) - 2*(1-x[0])
  der[-1] = 200*(x[-1]-x[-2]**2)
  
  return der

# problem dimension
n = 2

# num max iters
maxiter = 100000
# tolerancia
tol = 1e-10

# aprox. inicial
x = np.zeros(n)

siga = [x]
for k in range(maxiter):
  # direcao descendente
  d = -grad(x)

  # pesquisa linear
  alpha = spopt.line_search(fun, grad, x, d)[0]

  # atualizacao
  x = x + alpha * d

  nad = npla.norm(alpha * d)
  nfun = npla.norm(fun(x))

  if ((k+1) % 10 == 0):
    print(f"{k+1}: {alpha:1.2e} {nad:1.2e} {nfun:1.2e}")
    siga.append(x)

  if ((nfun < tol) or np.isnan(nfun)):
    break
  \end{lstlisting}
\end{ex}

\begin{exer}\label{exer:optFuns}
  Aplique o Método do Gradiente para resolver o problema de minimização com as seguintes funções objetivos:
  \begin{enumerate}[a)]
  \item Função de Beale
    \begin{equation}
      f(x,y) = (1.5-x+xy)^2 + (2.25-x+xy^2)^2 + (2.625-x+xy^3)^2
    \end{equation}
    Solução: f(3, 0.5) = 0.
  \item Função de Goldstein-Price
    \begin{align}
      f(x,y) &= \left[1+\left(x+y+1\right)^{2}\left(19-14x+3x^{2}-14y+6xy+3y^{2}\right)\right]\nonumber\\
      &\times \left[30+\left(2x-3y\right)^{2}\left(18-32x+12x^{2}+48y-36xy+27y^{2}\right)\right]
    \end{align}
    Solução: f(0,-1) = 3.
  \item Função de Booth
    \begin{equation}
      f(x,y) = \left( x + 2y -7\right)^{2} + \left(2x +y - 5\right)^{2}
    \end{equation}
    Solução: f(1,3)=0.
  \item Função de Rastringin:
    \begin{equation}
      f(x) = 10 n + \sum_{i=1}^n \left[x_i^2 - 10\cos(2 \pi x_i)\right]
    \end{equation}
    Solução: f(0)=0.
  \end{enumerate}
\end{exer}


\subsection{Método de Newton}

O Método de Newton{\newton} para problemas de otimização é um Método de Declive com direções descendentes
\begin{equation}
  d^{(k)} = -H^{-1}(x^{(k)})\nabla f(x^{(k)}),
\end{equation}
assumindo que a hessiana $H$ seja definida positiva dentro de uma vizinhança suficientemente grande do ponto de mínimo $x^*$.

\begin{obs}
  A cada iteração de Newton é necessário computar a inversa da matriz hessiana. Este é um passo crítico para o desempenho computacional. Desta forma, a escolha do método para o cálculo da inversa é normalmente explicitado, este é chamado de {\it solver} linear. Por exemplo, \emph{Newton-Krylov} é o nome dado ao Método de Newton que utiliza um Método de Subespaço de Krylov como {\it solver} linear. Mais especificamente, \emph{Newton-GMRES} quando a inversa é computada com o GMRES. Uma escolha natural é \emph{Newton-GC}, tendo em vista que o Método de Gradiente Conjugado é ideal para matriz simétrica e definida positiva.
\end{obs}

\begin{obs}
  Na implementação computacional não é necessário computar a inversa da hessiana, mais apenas computar $d^{(k)}$ resolvendo o seguinte sistema linear
  \begin{equation}
    H(x^{(k)})d^{(k)} = -\nabla f(x^{(k)}).
  \end{equation}
\end{obs}

\begin{ex}\label{ex:optNewtonGC}
  Seguindo o Exemplo \ref{ex:Rosenbrock}, temos que a hessiana associada é a matriz simétrica $H = [h_{i,j}]_{i,j=1}^{n,n}$ com
  \begin{align}
    h_{1,1} &= \frac{\p^2 f}{\p x_1^2} = 1200x_1^2-400x_2+2\\
    h_{1,2} &= \frac{\p^2 f}{\p x_1\p x_2} = -400x_1\\
            &~\nonumber\\
    h_{i,j} &= \frac{\p^2 f}{\p x_i\p x_j} = 200(\delta_{i,j}-2x_{i-1}\delta_{i-1,j}) - 400(\delta_{i+1,j}-2x_i\delta_{i,j})\nonumber\\
            &- 400\delta_{i,j}(x_{i+1}-x_i^2) + 2\delta_{i,j}\\
            &~\nonumber\\
    h_{n-1,n} &= \frac{\p^2 f}{\p x_{n-1}\p x_n} = -400x_{n-1}\\
    h_{n,n} &= \frac{\p^2 f}{\p x_{n-1}} = 200
  \end{align}
  Notemos que a hessiana é uma matriz tridiagonal.

  \lstinputlisting[caption=Algoritmo de Newton, label={lst:algOtimNewton}]{./cap_otimizacao/dados/pyNewton/main.py}  
\end{ex}

\begin{obs}{Métodos Tipo Newton}
  Métodos Tipo Newton são aqueles que utilizam uma aproximação para a inversa da matriz hessiana. Uma estratégia comumente aplicada, é a de atualizar a hessiana apenas em algumas iterações, baseando-se em uma estimativa da taxa de convergência. Na Seção \ref{cap_otimizacao_sec_tipoNewton}, exploramos esta técnica no contexto de resolução de sistemas não lineares.
\end{obs}

\begin{exer}
  Aplique o Método de Newton para minimizar a função de Rosenbrock no caso de várias dimensões. Para tanto, utilize uma implementação eficiente com suporte para matrizes esparsas. Compare o desempenho entre os métodos Newton-GMRES e Newton-GC. 
\end{exer}

\begin{exer}
  Aplique o Método de Newton para minimizar as funções dadas no Exercício \ref{exer:optFuns}.
\end{exer}

\subsection{Método do Gradiente Conjugado}

Métodos do Gradiente Conjugado são obtidos escolhendo-se as direções descendentes
\begin{equation}
  d^{(k)} = -\nabla f(x^{(k)}) + \beta_k d^{(k-1)},
\end{equation}
onde $\beta_k$ é um escalar escolhido de forma que as direções $\{d^{(k)}\}$ sejam mutuamente ortogonais com respeito a uma dada norma. Por exemplo, o \emph{Método de Fletcher}-\emph{Reeves} consiste em escolher
\begin{equation}
  \beta_k = \frac{\nabla f(x^{(k)})\cdot\nabla f(x^{(k)})}{\nabla f(x^{(k-1)})\cdot\nabla f(x^{(k-1)})},
\end{equation}
o que garante que as direções sejam mutuamente ortogonais com respeito ao produto interno euclidiano.

\begin{obs}
  Outras variações comumente empregadas são o Método de Polak-Ribière e suas variantes. Consulte mais em \cite[Seção 5.2]{Nocedal2006}.
\end{obs}

\begin{ex}
  Implementação do Método de Fletcher-Reeves para a minimização da função de Rosenbrock dada no Exemplo \ref{ex:Rosenbrock}.
  
  \lstinputlisting[caption=Algoritmo de Fletcher-Reeves, label={lst:algOtimGC}]{./cap_otimizacao/dados/pyMGC/main.py}    
\end{ex}

\begin{exer}
  Teste o Método de Fletcher-Reeves para a minimização das funções dadas no Exercício \ref{exer:optFuns}.
\end{exer}