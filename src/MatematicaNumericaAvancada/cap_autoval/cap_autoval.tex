%Este trabalho está licenciado sob a Licença Atribuição-CompartilhaIgual 4.0 Internacional Creative Commons. Para visualizar uma cópia desta licença, visite http://creativecommons.org/licenses/by-sa/4.0/deed.pt_BR ou mande uma carta para Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.

\chapter{Autovalores e Autovetores}\label{cap_autoval}
\thispagestyle{fancy}

\begin{flushright}
  [Vídeo] | [Áudio] | \href{https://phkonzen.github.io/notas/contato.html}{[Contatar]}
\end{flushright}

Neste capítulo, estudamos métodos numéricos para a computação de autovalores e autovetores de matrizes.

\section{Método da Potência}\label{cap_autoval_sec_pot}

O método da potência é uma técnica iterativa para computar o autovalor dominante de uma matriz e um autovetor associado. Modificações do método, tornam possível sua aplicação para a determinação de outros autovalores próximos a um dado número. Desta forma, pode ser utilizá-lo em conjunto com outra técnica de aproximação de autovalores e autovetores.

\subsection{Autovalor dominante}

Vamos denotar o \emph{espectro} de uma dada matriz $A\in\mathbb{C}^{n\times n}$ por
\begin{equation}
  \sigma(A) = \{\lambda_1, \lambda_2, \dotsc, \lambda_n\},
\end{equation}
sendo $\lambda_i\in \mathbb{C}$ o $i$-ésimo \emph{autovalor} de uma \emph{matriz diagonalizável}\footnote{Existe uma base de $\mathbb{C}^{n\times n}$ formada apenas de autovetores de $A$.} $A=[a_{i,j}]_{i,j=1}^{n,n}$, i.e. existe um vetor $0\neq v^{(i)}\in\mathbb{C}^n$ tal que
\begin{equation}
  Av^{(i)} = \lambda_i v^{(i)},
\end{equation}
sendo $v^{(i)}$ chamado de \emph{autovetor} associado a $\lambda_i$. No desenvolvimento do \emph{Método da Potência}, vamos assumir que os autovalores podem ser ordenados da seguinte forma
\begin{equation}
  |\lambda_1| > |\lambda_2| \geq \cdots \geq |\lambda_n|.
\end{equation}
Assim sendo, o $\lambda_1$ é chamado de \emph{autovalor dominante}. Na sequência, vamos denotar por $A^H$ a matriz adjunta\footnote{Também, chamada de matriz conjugada transposta de $A$.} de $A$, i.e. $A^H=[\bar{a}_{j,i}]_{i,j=1}^{n,n}$.

A \emph{iteração} do Método da Potência consiste em
\begin{align}
  z^{(k)} &= Aq^{(k-1)},\\
  q^{(k)} &= z^{(k)}/\left\|z^{(k)}\right\|,\\
  \nu_k &= (q^{(k)})^H A q^{(k)},
\end{align}
com dada aproximação inicial $q^{(0)}$ para o autovetor associado a $\lambda_1$. Quando o método é bem sucedido, tem-se $\nu_k\to \lambda_1$ quando $k\to\infty$.

Para mostrar a \emph{convergência} do método, basta mostrar que $q^{(k)}$ converge para um autovetor associado a $\lambda_1$. Primeiramente, notemos que\footnote{Segue por indução matemática.}
\begin{equation}
  q^{(k)} = \frac{A^kq^{(0)}}{\left\|A^kq^{(0)}\right\|},\quad k\geq 1.
\end{equation}
Como $A$ é diagonalizável, temos que existe uma base $\{v^{(1)}, v^{(2)}, \dotsc, v^{(n)}\}$ de $\mathbb{C}^{n\times n}$ formada apenas de autovetores de $A$. Segue que
\begin{equation}
  q^{(0)} = \sum_{i=1}^n \alpha_i v^{(i)},
\end{equation}
onde $\alpha_i\in\mathbb{C}$. Vamos assumir que $\alpha_1\neq 0$\footnote{Condição necessária para a convergência.}. Ainda, $Av^{(i)} = \lambda_i v^{(i)}$, donde
\begin{align}
  A^kq^{(0)} &= \sum_{i=1}^n \alpha_iA^kv^{(i)}\\
             &= \sum_{i=1}^n \alpha_i\lambda_i^kv^{(i)}\\
             &= \alpha_1\lambda_1^k\left[v^{(1)} + \sum_{i=2}^n\frac{\alpha_i}{\alpha_1}\left(\frac{\lambda_k}{\lambda_1}\right)^kv^{(i)}\right]
\end{align}
Como $|\lambda_i|/|\lambda_1| < 1$, $i=2,\dotsc,n$, temos que
\begin{equation}
  y^{(k)} = \sum_{i=2}^n\frac{\alpha_i}{\alpha_1}\left(\frac{\lambda_k}{\lambda_1}\right)^kv^{(i)} \to 0,
\end{equation}
quando $k\to \infty$. Logo, temos que
\begin{align}
  q^{(k)} &= \frac{\alpha_1\lambda_1^k\left(v^{(1)}+y^{(k)}\right)}{\left\|\alpha_1\lambda_1\left(v^{(1)}+y^{(k)}\right)\right\|}\\
          &= \frac{\lambda_1}{|\lambda_1|}\frac{\left(v^{(1)}+y^{(k)}\right)}{\left\|v^{(1)}+y^{(k)}\right\|}.
\end{align}
Por fim, observamos que $\lambda_1/|\lambda_1|$ tem o mesmo sinal de $\lambda_1$. Portanto, $q^{(k)}$ tende a um múltiplo não nulo de $v^{(1)}$ quando $k\to\infty$.

\begin{obs}[Taxa de convergência.]
  Pode-se mostrar a seguinte taxa de convergência para o Método da Potência
  \begin{equation}
    \left\|\tilde{q}^{(k)}-v^{(1)}\right\| \leq C\left|\frac{\lambda_2}{\lambda_1}\right|^k,\quad k\geq 1,
  \end{equation}
  onde
  \begin{equation}
    \tilde{q}^{(k)} = v^{(1)} + \sum_{i=2}^n \frac{\alpha_i}{\alpha_1}\left(\frac{\lambda_i}{\lambda_1}\right)^kv^{(i)}.
  \end{equation}
  Consulte \cite[Seção 5.3.]{Quarteroni2007a}.
\end{obs}

\lstinputlisting[caption=Algoritmo do Método da Potência, label={lst:algMetPot}]{./cap_autoval/dados/pyMetPot/main.py}

\begin{exer}
  Use o Método da Potência para computar o autovalor dominante da matriz de coeficientes do problema discreto associado ao Exercício \ref{exer:pvc1d}. Estude sua convergência para diferentes tamanhos da matriz. 
\end{exer}

\begin{exer}
  Use o Método da Potência para computar o autovalor dominante da matriz de coeficientes do problema discreto associado ao Exemplo \ref{ex:poisson}. Estude sua convergência para diferentes tamanhos da matriz. 
\end{exer}

\subsection{Método da Potência Inverso}

Esta variação do Método da Potência nos permite computar o autovalor mais próximo de um dado número $\mu\in\mathbb{C}$, $\mu\not\in\sigma(A)$. A ideia é aplicar o método para a matriz
\begin{equation}
  M_\mu^{-1} = (A-\mu I)^{-1}.
\end{equation}
Neste contexto, $\mu$ é chamado de \emph{deslocamento} (em inglês, {\it shift}).

Notemos que se $(\lambda_i, v_i)$ é um autopar de $A$, então $\xi_i=(\lambda_i-\mu)^{-1}$ é autovalor de $M_\mu^{-1}$ associado ao autovetor $v_i$. De fato,
\begin{gather}
  (\lambda_i - \mu) v_i = (A-\mu I)v_i\\
  M_\mu^{-1}(\lambda_i-\mu)v_i = v_i\\
  M_\mu^{-1}v_i = (\lambda_i-\mu)^{-1}v_i.
\end{gather}
Isso também mostra que $A$ e $M_\mu^{-1}$ têm os mesmos autovetores.

Agora, se $\mu$ for suficientemente próximo de $\lambda_m$, autovalor simples de $A$, então
\begin{equation}
  |\lambda_m-\mu| < |\lambda_i-\mu|,\quad i=1,2,\dotsc,n,~i\neq m.
\end{equation}
Com isso, $\xi_i=(\lambda_m-\mu)^{-1}$ é o autovalor dominante de $M_\mu^{-1}$ e, portanto, a iteração do Método da Potência aplicada a $M_\mu^{-1}$ fornece este autovalor. Mais especificamente, como $A$ e $M_\mu^{-1}$ tem os  mesmos autovetores, a \emph{iteração do Método da Potência Inverso} é dada como segue
\begin{gather}
  (A-\mu I)z^{(k)} = q^{(k-1)}, \label{eq:slmpi}\\
  q^{(k)}=z^{(k)}/\|z^{(k)}\|,\\
  \nu_k = (q^{(k)})^H A q^{(k)}.
\end{gather}


\begin{exer}
  Use o Método da Potência para computar diferentes autovalores da matriz de coeficientes do problema discreto associado ao Exercício \ref{exer:pvc1d}. Estude sua convergência para diferentes tamanhos da matriz. 
\end{exer}

\begin{exer}
  Use o Método da Potência para computar diferentes autovalores da matriz de coeficientes do problema discreto associado ao Exemplo \ref{ex:poisson}. Verifique se há vantagem em aplicar os métodos GMRES e GC na resolução de \eqref{eq:slmpi}.
\end{exer}

\section{Iteração QR}\label{cap_autoval_sec_qr}

A iteração QR é um método para a computação aproximada de todos os autovalores de uma dada matriz $A$. A ideia é explorar um método iterativo para a computação da \emph{decomposição de Schur}\footnote{Issai Schur, 1875 - 1941, matemático russo-alemão. Fonte: \href{https://pt.wikipedia.org/wiki/Issai_Schur}{Wikipédia}.} de A, i.e. encontrar uma \emph{matriz unitária} $U$\footnote{Uma matriz $U$ é dita unitária quando $U^{-1} = U^H$.} tal que
\begin{equation}
  U^{H}AU =
  \begin{bmatrix}
    \lambda_1 & t_{12} & \cdots & t_{1n}\\
    0 & \lambda_2 & & t_{2n}\\
    \vdots & & \ddots & \vdots\\
    0 & \vdots & 0 & \lambda_n
  \end{bmatrix}
\end{equation}

Assumindo $A\in\mathbb{R}^{n\times n}$, sejam $Q^{(0)}\in\mathbb{R}^{n\times n}$ uma \emph{matriz ortogonal}\footnote{Uma matriz $Q$ é dita ortogonal quando $Q^TQ = I$.} e $T^{(0)} = (Q^{(0)})^TAQ^{(0)}$. A iteração $QR$ consiste em
\begin{align}
  &\text{determinar }Q^{(k)}, R^{(k)}\text{ tal que}\nonumber\\
  &Q^{(k)}R^{(k)} = T^{(k-1)}\qquad\text{(fatoração QR)}\\
  &T^{(k)} = R^{(k)}Q^{(k)},
\end{align}
para $k=1,2,\ldots$.

Ou seja, a cada iteração $k$, computa-se a fatoração da matriz $T^{(k-1)}$ como o produto de uma matriz ortogonal $Q^{(k)}$ com uma matriz triangular superior $R^{(k)}$. Então, computa-se uma nova aproximação $T^{(k)}$ pela multiplicação de $R^{(k)}$ por $Q^{(k)}$. Com isso, temos
\begin{align}
  T^{(k)} &= R^{(k)}Q^{(k)}\\
          &= (Q^{(k)})^T(Q^{(k)}R^{(k)})Q^{(k)}\\
          &= (Q^{(k)})^TT^{(k-1)}Q^{(k)}\\
          &= (Q^{(k)})^TR^{(k-1)}Q^{(k-1)}Q^{(k)}\\
          &= (Q^{(k)})^T(Q^{(k-1)})^TQ^{(k-1)}R^{(k-1)}Q^{(k-1)}Q^{(k)}\\
          &= (Q^{(k-1)}Q^{(k)})^TT^{(k-2)}(Q^{(k-1)}Q^{(k)})\\
          &=(Q^{(0)}\cdots Q^{(k)})^TA(Q^{(0)}\cdots Q^{(k)}).
\end{align}

\begin{obs}[Convergência do método QR]
  Se $A\in\mathbb{R}^{n\times n}$ for uma matriz com autovalores tais que
  \begin{equation}
    |\lambda_1| > |\lambda_2| > \cdots > |\lambda_n|,
  \end{equation}
  então
  \begin{equation}
    \lim_{k\to\infty} T^{(k)} =   \begin{bmatrix}
    \lambda_1 & t_{12} & \cdots & t_{1n}\\
    0 & \lambda_2 & & t_{2n}\\
    \vdots & & \ddots & \vdots\\
    0 & \vdots & 0 & \lambda_n
  \end{bmatrix}.
\end{equation}

  Para a taxa de convergência, temos
  \begin{equation}
    |t^{(k)}_{i,i-1}| = \mathcal{O}\left(\left|\frac{\lambda_i}{\lambda_{i-1}}\right|^k\right),\quad i=2,\dotsc,n,
  \end{equation}
  quando $k\to\infty$.

  Ainda, se $A$ for uma \emph{matriz simétrica}, então $T^{(k)}$ tende a uma matriz diagonal quando $k\to\infty$.
\end{obs}

\begin{obs}
  Variantes do método QR permitem sua aplicação para matrizes mais gerais. Consulte \cite{Golub2013a}.
\end{obs}

Uma forma de eficiente do método QR é chamada de \emph{iteração Hessenberg}\footnote{Karl Adolf Hessenberg, 1904 - 1959, matemático e engenheiro alemão. Fonte: \href{https://pt.wikipedia.org/wiki/Karl_Hessenberg}{Wikipédia}.}\emph{-QR}. Consiste em inicializar $T^{(0)}$ como uma \emph{matriz de Hessenberg superior}, i.e. $t^{(0)}_{i,j}=0$ para $i>j+1$. A computação de $T^{(0)}$ é feito com \emph{matrizes de Householder}\footnote{Alston Scott Householder, 1904 - 1993, matemático estadunidense. Fonte: \href{https://pt.wikipedia.org/wiki/Alston_Scott_Householder}{Wikipédia}.} e a fatoração $QR$ de $T^{(k)}$ utiliza de \emph{matrizes de Givens}\footnote{James Wallace Givens Jr., 1910 - 1993, matemático estadunidense. Fonte: \href{https://pt.wikipedia.org/wiki/Wallace_Givens}{Wikipédia}.}.

\lstinputlisting[caption=Algoritmo da Iteração QR, label={lst:pyQRIter}]{./cap_autoval/dados/pyQRIter/main.py}

\begin{obs}[Computação dos autovalores]
  Se $v$ é autovetor associado ao autovalor simples $\lambda$ de $A$, então
  \begin{align}
    Av &= \lambda v\\
    (Q^{(k)})^TAQ^{(k)}(Q^{(k)})^Tv &\approx \lambda (Q^{(k)})^Tv
  \end{align}
  Denotando, $y = (Q^{(k)})^Tv$, temos
  \begin{equation}
    T^{(k)}y = \lambda y.
  \end{equation}

  Com isso, podemos computar $y$ e, então, temos $v\approx Q^{(k)}y$.
\end{obs}


\begin{exer}
  Use a iteração QR para computar os autovalores da matriz de coeficientes do problema discreto associado ao Exercício \ref{exer:pvc1d}.
\end{exer}

\begin{exer}
  Use a iteração QR para computar os autovalores da matriz de coeficientes do problema discreto associado ao Exemplo \ref{ex:poisson}. Use \lstinline+spla.hessenberg+ para computar $T^{(0)}$.
\end{exer}
