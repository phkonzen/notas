

\chapter{Computação paralela e distribuída (MPI)}\label{cap_mpi}
\thispagestyle{fancy}
\badge[yellow]{Em revisão}

Neste capítulo, vamos estudar aplicações da computação paralela em arquitetura de memória distribuída. Para tanto, vamos utilizar códigos C/C++ com a API \href{https://www.open-mpi.org/}{Open MPI}.

\section{Olá, Mundo!}\label{cap_mpi_sec_ola}
\badge[yellow]{Em revisão}

A computação paralela com MPI inicia-se simultaneamente com múltiplos processadores (instâncias de processamento), cada um utilizando seu próprio endereço de memória (memória distribuída). Cada processo lê e escreve em seu próprio endereço de memória privada. Observamos que o processamento já inicia-se ramificado e distribuído, sendo possível a comunicação entre os processos por instruções explícitas (instruções MPI, {\it Message Passing Interface}). A sincronização entre os processos também requer instruções específicas.

Vamos escrever nosso primeiro código MPI. O Código \verb+ola.cc+ é paralelamente executado por diferentes processadores, cada processo escreve ``Olá'' e identifica-se.

\lstinputlisting[title={Código: ola.cc}]{cap_mpi/dados/cc_ola/ola.cc}

Na linha 3, o API MPI é incluído no código. O ambiente MPI é inicializado na linha 8 com a rotina \href{https://www.open-mpi.org/doc/v4.1/man3/MPI\_Init.3.php}{MPI\_Init} inicializa o ambiente MPI. Na inicialização, o comunicador \verb+MPI_COMM_WORLD+ é construído entre todos os processos inicializados e um identificador ({\it rank}) é atribuído a cada processo. O número total de processos é obtido com a rotina \href{https://www.open-mpi.org/doc/v4.1/man3/MPI\_Comm\_size.3.php}{MPI\_Comm\_size}. Cada processo é identificado por um número natural sequencial 0, 1, ..., \verb+world_size+-1. O id ({\it rank}) de um processo é obtido com a rotina \href{https://www.open-mpi.org/doc/v4.1/man3/MPI\_Comm\_rank.3.php}{MPI\_Comm\_rank} (veja a linha 16). A rotina \href{https://www.open-mpi.org/doc/v4.1/man3/MPI\_Finalize.3.php}{MPI\_Finalize} finaliza o ambiente MPI.


Para compilar este código, digite no terminal
\begin{verbatim}
$ mpic++ ola.cc
\end{verbatim}
Esta instrução de compilação é análoga a
\begin{verbatim}
g++ ola.cc -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi
   -I/usr/lib/x86_64-linux-gnu/openmpi/include 
   -pthread -L/usr/lib/x86_64-linux-gnu/openmpi/lib 
   -lmpi_cxx -lmpi
\end{verbatim}
ou semelhante dependendo da instalação. Para ver a sua configuração, digite
\begin{verbatim}
$ mpic++ ola.cc --showme
\end{verbatim}


Ao compilar, um executável \verb+a.out+ será criado. Para executá-lo, basta digitar no terminal:
\begin{verbatim}
$ mpirun -np 2 a.out
\end{verbatim}
Esta instrução inicializa simultaneamente duas cópias (\verb+-np 2+, dois processos) do código \verb+ola.cc+ (do executável \verb+a.out+). Cada processo é executado de forma idependente (em paralelo e não sincronizados).

Ao executar, devemos ver a saída do terminal como algo parecido com
\begin{verbatim}
Olá! Eu sou o processo 1/2.
Olá! Eu sou o processo 0/2.
\end{verbatim}

A saída irá variar conforme o processo que primeiro enviar a mensagem para o dispositivo de saída. Execute o código várias vezes e analise as saídas!

\subsection{Exercícios resolvidos}
\badge[yellow]{Em revisão}

\begin{exeresol}
  O número de instâncias de processamento pode ser alterado diretamente na instrução \verb+mpirun+ pela opção \verb+-np+. Altere o número de instâncias de processamento para 4 e execute o Código ola.cc.
\end{exeresol}
\begin{resol}
  Para alterar o número de instâncias de processamento não é necessário recompilar o código\footnote{Caso ainda não tenha compilado o código, copile-o.}. Basta executá-lo com o comando
\begin{verbatim}
$ mpirun -np 4 ./a.out
\end{verbatim}
  A saída deve ser algo do tipo
\begin{verbatim}
Olá! Eu sou o processo 1/4.
Olá! Eu sou o processo 3/4.
Olá! Eu sou o processo 2/4.
Olá! Eu sou o processo 0/4.
\end{verbatim}
  Execute o código várias vezes e analise as saídas!
\end{resol}

\begin{exeresol}
  Escreva um código MPI para ser executado com 2 instâncias de processamento. Cada processo recebe os números inteiros
\begin{verbatim}
int n = 2;
int m = 3;
\end{verbatim}
Então, um dos processos deve escrever a soma $n+m$ e o outro deve escrever o produto.
\end{exeresol}
\begin{resol}
  O código abaixo contém uma implementação deste exercício. Veja os comentários abaixo.
  \lstinputlisting[title={Código: sp.cc}]{cap_mpi/dados/cc_exeresol_sp/sp.cc}

  Neste código, os processos são abortados caso o usuário tente executá-lo com um número de processos diferente de 2. Para abortar todos os processos ativos, utiliza-se a rotina \href{https://www.open-mpi.org/doc/v4.1/man3/MPI\_Abort.3.php}{MPI\_Abort} (veja as linhas 15-21). O argumento de entrada \verb+errorcode+ é arbitrário e serve para informar o usuário de uma categoria de erros conforme a política de programação utilizada.

  Observamos que o controle do que cada processo deve fazer, é feito através de sua identificação \verb+world_rank+ (veja as linhas 30-33).
\end{resol}

\subsection{Exercícios}
\badge[yellow]{Em revisão}

\begin{exer}
  Rode o Código \verb+ola.cc+ com um número de processadores ({\it core}) maior do que o disponível em sua máquina. O que você observa? Modifique a instrução \verb+mpirun+ para aceitar a execução confirme o número de \textit{threads} disponível na máquina. Por fim, modifique a instrução de forma a aceitar um número arbitrário de instâncias de processamento.
\end{exer}

\begin{exer}
  Faça um código MPI para ser executado com 2 instâncias de processamento. Uma das instâncias de processamento deve alocar
\begin{verbatim}
int a = 2;
int b = 3;
\end{verbatim}
e escrever a diferença $a-b$. A outra instância deve alocar
\begin{verbatim}
int a = 4;
int b = 5;
\end{verbatim}
e escrever o quociente $b/a$.
\end{exer}

\section{Rotinas de comunicação ponto-a-ponto}\label{cap_mpi_sec_p2pcom}
\badge[yellow]{Em revisão}

Em computação distribuída, rotinas de comunicação entre as instâncias de processamento são utilizadas para o compartilhamento de dados. Neste capítulo, vamos discutir sobre as rotinas de comunicação ponto-a-ponto, i.e. comunicações entre uma instância de processamento com outra.

\subsection {Envio e recebimento síncronos}
\badge[yellow]{Em revisão}

O envio e recebimento de dados entre duas instâncias de processamento pode ser feita com as rotinas \href{https://www.open-mpi.org/doc/current/man3/MPI\_Send.3.php}{MPI\_Send} e \href{https://www.open-mpi.org/doc/current/man3/MPI\_Recv.3.php}{MPI\_Recv}. A primeira é utilizada para o envio de um dado a partir de uma instância de processamento e a segunda é utilizada para o recebimento de um dado em uma instância de processamento.

A sintaxe da \href{https://www.open-mpi.org/doc/current/man3/MPI\_Send.3.php}{MPI\_Send} é
\begin{verbatim}
int MPI_Send(
  const void *buf, 
  int count, 
  MPI_Datatype datatype, 
  int dest,
  int tag, 
  MPI_Comm comm)
\end{verbatim}
e a sintaxe da \href{https://www.open-mpi.org/doc/current/man3/MPI\_Send.3.php}{MPI\_Recv} é
\begin{verbatim}
int MPI_Recv(
  void *buf, 
  int count, 
  MPI_Datatype datatype,
  int source, 
  int tag, 
  MPI_Comm comm, 
  MPI_Status *status)
\end{verbatim}

O primeiro argumento é o ponteiro do {\it buffer} de dados. No caso do \verb+MPI_Send+ é o ponteiro para a posição da memória do dado a ser enviado. No caso do \verb+MPI_Recv+ é o ponteiro para a posição da memória do dado a ser recebido. O segundo argunto \verb+count+ é o número de dados sequenciais a serem enviados. O argundo \verb+datatype+ é o tipo de dado. O MPI suporta os seguintes tipos de dados
\begin{verbatim}
MPI_SHORT               short int
MPI_INT                 int
MPI_LONG                long int
MPI_LONG_LONG           long long int
MPI_UNSIGNED_CHAR       unsigned char
MPI_UNSIGNED_SHORT      unsigned short int
MPI_UNSIGNED            unsigned int
MPI_UNSIGNED_LONG       unsigned long int
MPI_UNSIGNED_LONG_LONG  unsigned long long int
MPI_FLOAT               float
MPI_DOUBLE              double
MPI_LONG_DOUBLE         long double
MPI_BYTE                char
\end{verbatim}

Ainda sobre as sintaxes acima, o argumento \verb+source+ é o identificador {\it rank} da instância de processamento. O argunmento \verb+tag+ é um número arbitrário para identificar a operação de envio e recebimento. O argumento \verb+Comm+ especifica o comunicador (\verb+MPI_COMM_WORLD+ para aplicações básicas) e o último (somente para o \verb+MPI_Recv+) fornece informação sobre o {\it status} do recebimento do dado.

Vamos estudar o seguinte código abaixo.

\lstinputlisting[title={Código: sendRecv.cc}]{cap_mpi/dados/cc_sendRecv/sendRecv.cc}

O código acima pode rodado com pelo menos duas instâncias de processamento (veja as linhas 14-19). Nas linhas 28-29, o processo 0 envia o número \verb+3.1416+ (alocado na variável \verb+x+) para o processo 1. Nas linhas 32-33, o processo 1 recebe o número enviado pelo processo 0 e o aloca na variável \verb+y+.

{\bf Importante!} As rotinas \verb+MPI_Send+ e \verb+MPI_Recv+ provocam a sincronização entre os processos envolvidos. Por exemplo, no código acima, no que o processo 0 atinge a rotina \verb+MPI_Send+ ele ficará aguardando o processo 1 receber todos os dados enviados e só, então, irá seguir adiante no código. Analogamento, no que o processo 1 atingir a rotina \verb+MPI_Recv+, ele ficará aguardando o processo 0 enviar todos os dados e só, então, irá seguir adiante no código.

\subsubsection{Envio e recebimento de {\it array}}
\badge[yellow]{Em revisão}

As rotinas \verb+MPI_Send+ e \verb+MPI_Recv+ podem ser utilizadas para o envio e recebimento de {\it arrays}. A sintaxe é a mesma vista acima, sendo que o primeiro argumento \verb+*buf+ deve apontar para o início do {\it array} e o segundo argumento \verb+count+ corresponde ao tamanho da {\it array}.

Vamos estudar o seguinte código. Nele, o processo 0 aloca $v = (0,1,2,3,4)$ e o processo 1 aloca $w = (4,3,2,1,0)$. O processo 0 envia os valores $v_1, v_2, v_3$ para o processo 1. Então, o processo 1 recebe estes valores e os aloca em $w_0, w_1, w_2$. Desta forma, a saída impressa no terminal é
\begin{equation}
  w = (1, 2, 3, 1, 0).
\end{equation}
Verifique!

\lstinputlisting[title={Código: sendRecvArray.cc}]{cap_mpi/dados/cc_sendRecvArray/sendRecvArray.cc}


\subsection{Envio e recebimento assíncrono}
\badge[yellow]{Em revisão}

O \verb+MPI+ também suporta rotinas \href{https://www.open-mpi.org/doc/current/man3/MPI_Isend.3.php}{MPI\_Isend} de envio e \href{https://www.open-mpi.org/doc/current/man3/MPI_Irecv.3.php}{MPI\_Irecv} de recebimento assíncronos. Neste caso, o processo emissor envia o dado para outro processo e segue imediatamente a computação. O processo receptor deve conter uma rotina \href{https://www.open-mpi.org/doc/current/man3/MPI_Irecv.3.php}{MPI\_Irecv}, mas também não aguarda sua conclusão para seguir a computação.

As sintaxes destas rotinas são semelhantes as das rotinas \verb+MPI_Send+ e \verb+MPI_Recv+.

\begin{verbatim}
int MPI_Isend(
  const void *buf, 
  int count, 
  MPI_Datatype datatype, 
  int dest,
  int tag, MPI_Comm comm, 
  MPI_Request *request)
\end{verbatim}

\begin{verbatim}
int MPI_Irecv(
  void *buf, 
  int count, 
  MPI_Datatype datatype,
  int source, 
  int tag, 
  MPI_Comm comm, 
  MPI_Request *request)
\end{verbatim}
O último argumento permite verificar os envios e recebimentos.

Vamos estudar o seguinte código.

\lstinputlisting[title={Código: isendRecv.cc}]{cap_mpi/dados/cc_IsendRecv/isendRecv.cc}

Neste código, \verb+MPI_Status+ e \verb+MPI_Request+ são alocados nas linhas 26 e 27, respectivamente. O Processo 0 faz uma requisição de envio do número $3.1416$ para o processo 1, não aguarda o recebimento e segue adiante. O processo 1 tem uma rotina de requisição de recebimento não assíncrona na linha 35. Neste momento, ele não necessariamente recebe o dado enviado pelo processador (isto pode ocorrer a qualquer momento mais adiante). Na linha 37, o valor de \verb+y+ deve ainda ser $0.0$, veja a saída do código.

\begin{verbatim}
$ mpic++ isendRecv.cc
$ $ mpirun -np 2 ./a.out
x = 1.000000
x = 4.141600
\end{verbatim}

Pode-se verificar se uma requisição de envio (ou recebimento) foi completata usando-se a rotina \href{https://www.open-mpi.org/doc/current/man3/MPI\_Test.3.php}{MPI\_Test}. A sua sintaxe é
\begin{verbatim}
int MPI_Test(
  MPI_Request *request, 
  int *flag, 
  MPI_Status *status)
\end{verbatim}
O \verb+flag == 0+ caso a requisição ainda não foi completada e \verb+flag == 1+ caso a requisição foi executada.

No Código \verb+isendRecv.cc+ acima, as linhas de código 39-41 são utilizadas para fazê-lo aguardar até que a requisição de recebimento seja completada. Desta forma, na linha 42 o valor de \verb+y+ é $3.1416$ (o valor enviado pelo processo 0. Verifique!

\begin{obs}
  No Código \verb+isendRecv.cc+ acima, as linhas 39-41 podem ser substituídas pela rotina \href{https://www.open-mpi.org/doc/current/man3/MPI\_Wait.3.php}{MPI\_Wait}, a qual tem sintaxe
\begin{verbatim}
int MPI_Wait(
  MPI_Request *request, 
  MPI_Status *status)
\end{verbatim}
  Verifique!
\end{obs}

\subsection{Exercícios}
\badge[yellow]{Em revisão}

\begin{exer}
  Faça um código MPI para ser executado com 2 processadores. Um processo aloca $x = 0$ e o outro processo aloca $y=1$. Logo, os processos trocam os valores, de forma que ao final o processo zero tem $x = 1$ e o processo 1 tem $y=0$.
\end{exer}

\begin{exer}
  Faça um código MPI para ser executado com 2 processadores. O processo 0 aloca um vetor de $n\geq 1$ elementos randômicos em ponto flutuante, envia o vetor para o processo 1. O processo 0, imprime no terminal a soma dos termos do vetor e o processo 1 imprime o produto dos termos do vetor.
\end{exer}


\begin{exer}
  Faça um código MPI para computar a média
  \begin{equation}
    \frac{1}{n}\sum_{i=0}^{n-1} x_i
  \end{equation}
  onde $x_i$ é um número em ponto flutuante e $n\geq 1$. Para a comunicação entre os processos, utilize apenas as rotinas \verb+MPI_Send+ e \verb+MPI_Recv+.
\end{exer}

\begin{exer}\label{exer:mpi_prodin}
  Faça um código MPI para computação do produto interno entre dois vetores
  \begin{gather}
    x = (x_0, x_1, \dotsc, x_n),\\
    y = (y_0, y_1, \dotsc, y_n).
  \end{gather}
  Para a comunicação entre os processos, utilize apenas as rotinas \verb+MPI_Send+ e \verb+MPI_Recv+. O processo 0 deve receber os resultados parciais dos demais processos e escrever na tela o valor computado do produto interno.
\end{exer}

\begin{exer}
  Modifique o código do exercício anterior (Exercício \ref{exer:mpi_prodin}) de forma a fazer a comunicação entre os processos com as rotinas \verb+MPI_Isend+ e \verb+MPI_Irecv+. Há vantagem em utilizar estas rotinas? Se sim, quais?
\end{exer}

\section {Comunicações coletivas}\label{cap_mpi_sec_colcom}
\badge[yellow]{Em revisão}

Nesta seção, vamos discutir sobre rotinas de comunicações MPI coletivas. Basicamente, rotinas de sincronização, envio e recebimento de dados envolvendo múltiplas instâncias de processamento ao mesmo tempo.

\subsection {Barreira de sincronização}
\badge[yellow]{Em revisão}

Podemos forçar a sincronização de todos os processos em um determinado ponto do código utilizando a rotina de sincronização \href{https://www.open-mpi.org/doc/current/man3/MPI\_Barrier.3.php}{MPI\_Barrier}
\begin{verbatim}
int MPI_Barrier (MPI_Comm comm)
\end{verbatim}
Quando um processo encontra esta rotina ele aguarda todos os demais processos. No momento em que todos os processo tiverem alcançados esta rotina, todos são liberados para seguirem com suas computações.

\begin{ex}
  No Código \verb+barrier.cc+, abaixo, cada instância de processamento aguarda randomicamente até 3 segundos para alcançar a rotina de sincronização \href{https://www.open-mpi.org/doc/current/man3/MPI\_Barrier.3.php}{MPI\_Barrier} na linha 35. Em seguida, elas são liberadas juntas. Estude o código.

\lstinputlisting[title={Código: barrier.cc}]{cap_mpi/dados/cc_Barrier/barrier.cc}

Vamos observar o seguinte teste de rodagem
\begin{verbatim}
$ mpic++ barrier.cc
$ mpirun -np 2 a.out                                                          
1 chegou na barreira: 1 s.
0 chegou na barreira: 3 s.
0 saiu da  barreira: 3 s.
1 saiu da  barreira: 3 s.
\end{verbatim}
Neste caso, o processo 1 foi o primeiro a alcançar a barreira de sincronização e permaneceu esperando aproximadamente 2 segundos até que o processo 0 alcançasse a barreira. Imediatamente após o processo 1 chegar a barreira, ambos seguiram suas computações. Rode várias vezes este código e analise as saídas!
\end{ex}

\begin{obs}
  No Código \verb+barrier.cc+ acima, o gerador de números randômicos é inicializado com a semente
\begin{verbatim}
srand (init + world_rank);
\end{verbatim}
  onde, \verb+init+ é o tempo colhido pela rotina \verb+time+ no início do processamento (veja as linhas 22-25). Observamos que somar o identificado \verb+rank+ garante que cada processo inicie o gerador randômico com uma semente diferente.
\end{obs}

\subsection {Transmissão coletiva}
\badge[yellow]{Em revisão}

A rotina de transmissão de dados \href{https://www.open-mpi.org/doc/current/man3/MPI\_Bcast.3.php}{MPI\_Bcast} permite o envio de dados de um processo para todos os demais. Sua sintaxe é a seguinte
\begin{verbatim}
int MPI_Bcast(
  void *buffer, 
  int count, 
  MPI_Datatype datatype,
  int root, 
  MPI_Comm comm)
\end{verbatim}
O primeiro argumento \verb+buffer+ aponta para o endereço da memória do dado a ser transmitido. O argumento \verb+count+ é a quantidade de dados sucessivos que serão transmitidos (tamanho do \verb+buffer+). O tipo de dado é informado no argumento \verb+datatype+. Por fim, \verb+root+ é o identificador \verb+rank+ do processo que está transmitindo e \verb+comm+ é o comunicador.

\begin{ex}
  No seguinte Código \verb+bcast.cc+, o processo 0 inicializa a variável de ponto flutuante $x = \pi$ (linhas 22-23) e, então, transmite ela para todos os demais processos (linhas 25-26). Por fim, cada processo imprime no terminal o valor alocado na sua variável $x$ (linhas 28-29).

\lstinputlisting[title={Código: bcast.cc}]{cap_mpi/dados/cc_bcast/bcast.cc}

Vejamos o seguinte teste de rodagem
\begin{verbatim}
$ mpic++ bcast.cc
$ mpirun -np 3 ./a.out                                                           
Processo 0 x = 3.141593
Processo 1 x = 3.141593
Processo 2 x = 3.141593
\end{verbatim}
\end{ex}

\subsection {Distribuição coletiva de dados}
\badge[yellow]{Em revisão}

A rotina \href{https://www.open-mpi.org/doc/current/man3/MPI\_Scatter.3.php}{MPI\_Scatter} permite que um processo faça a distribuição uniforme de pedaços sequenciais de um {\it array} de dados para todos os demais processos. Sua sintaxe é a seguinte
\begin{verbatim}
int MPI_Scatter(
  const void *sendbuf, 
  int sendcount, 
  MPI_Datatype sendtype,
  void *recvbuf, 
  int recvcount, 
  MPI_Datatype recvtype, 
  int root,
  MPI_Comm comm)
\end{verbatim}
O primeiro argumento \verb+sendbuf+ aponta para o endereço de memória do {\it array} de dados a ser distribuído. O argumento \verb+sendcount+ é o tamanho do pedaço e \verb+sendtype+ é o tipo de dado a ser transmitido. Os argumentos \verb+recvbuf+, \verb+recvcount+ e \verb+recvtype+ se referem ao ponteiro para o local de memória onde o dado recebido será alocado, o tamanho do pedaço a ser recebido e o tipo de dado, respectivamente. Por fim, o argumento \verb+root+ identifica o processo de origem da distribuição dos dados e \verb+comm+ é o comunicador.

\begin{ex}
  No Código \verb+scatter.cc+ abaixo, o processo 0 aloca o vetor
  \begin{equation}
    v = (1, 2, \dotsc, 10),
  \end{equation}
  distribui pedaços sequenciais do vetor para cada processo no comunicador \verb+MPI_COMM_WORLD+ e, então, cada processo computa a soma dos elementos recebidos.

\lstinputlisting[title={Código: scatter.cc}]{cap_mpi/dados/cc_scatter/scatter.cc}

  Vejamos o seguinte teste de rodagem
\begin{verbatim}
$ mpic++ scatter.cc -lgsl -lgslcblas
$ mpirun -np 2 ./a.out                                           
Processo 0 soma = 15.000000
Processo 1 soma = 40.000000
\end{verbatim}
  Neste caso, o processo 0 recebe
  \begin{equation}
    \verb+my_v+ = (1, 2, 3, 4, 5),
  \end{equation}
  enquanto o processo 1 recebe
  \begin{equation}
    \verb+my_v+ = (6, 7, 8, 9, 10).
  \end{equation}
\end{ex}

\begin{obs}
  Note que o \href{https://www.open-mpi.org/doc/current/man3/MPI\_Scatter.3.php}{MPI\_Scatter} distribuí apenas pedaços de {\it arrays} de mesmo tamanho. Para a distribuição de pedaços de tamanhos diferentes entre os processos, pode-se usar a rotina \href{https://www.open-mpi.org/doc/current/man3/MPI\_Scatterv.3.php}{MPI\_Scatterv}. Veja os exercícios resolvidos abaixo.
\end{obs}

\subsection {Recebimento coletivo de dados distribuídos}
\badge[yellow]{Em revisão}

A rotina \href{https://www.open-mpi.org/doc/current/man3/MPI\_Gather.3.php}{MPI\_Gather}, permite que um processo receba simultaneamente dados que estão distribuídos entre os demais processos. Sua sintaxe é a seguinte
\begin{verbatim}
int MPI_Gather(
  const void *sendbuf, 
  int sendcount, 
  MPI_Datatype sendtype,
  void *recvbuf, 
  int recvcount, 
  MPI_Datatype recvtype, 
  int root,
  MPI_Comm comm)
\end{verbatim}
Sua sintaxe é parecida com a da rotina \href{https://www.open-mpi.org/doc/current/man3/MPI\_Scatter.3.php}{MPI\_Scatter}. Veja lá! Aqui, \verb+root+ é o identificador {\it rank} do processo receptor.

\begin{ex}
  No Código \verb+gather.cc+, cada processo $i$ aloca um vetor
  \begin{equation}
    \verb+my_v+ = (5i+1, 5i+2, \dotsc, 5i+5),
  \end{equation}
  então, o processo 0 recebe estes vetores alocando-os em um único vetor
  \begin{equation}
    \verb+v+ = (1, 2, \dotsc, 5n_p),
  \end{equation}
  onde $n_p$ é o número de processos inicializados.

  \lstinputlisting[title={Código: gather.cc}]{cap_mpi/dados/cc_gather/gather.cc}
  
  Vejamos o seguinte teste de rodagem
\begin{verbatim}
$ mpic++ gather.cc -lgsl -lgslcblas
$ mpirun -np 2 ./a.out                                             
v = 1.000000 2.000000 3.000000 4.000000 5.000000 
6.000000 7.000000 8.000000 9.000000 10.000000
\end{verbatim}
  Neste caso, o processo  0 aloca
  \begin{equation}
    \verb+my_v+ = (1, 2, 3, 4, 5)
  \end{equation}
  e o processo 1 aloca
  \begin{equation}
    \verb+my_v+ = (6, 7, 8, 9, 10).
  \end{equation}
  Então, o processo 0, recebe os dois pedaços de cada um, formando o vetor
  \begin{equation}
    \verb+v+ = (1, 2, \dotsc, 10).
  \end{equation}
  Verifique!
\end{ex}

\begin{obs}
  Para recebimento de pedaços distribuídos e de tamanhos diferentes, pode-se usar a rotina \href{https://www.open-mpi.org/doc/current/man3/MPI\_Gatherv.3.php}{MPI\_Gatherv}. Veja os exercícios resolvidos abaixo.
\end{obs}

\begin{obs}
  Observamos que com a rotina \href{https://www.open-mpi.org/doc/current/man3/MPI\_Gather.3.php}{MPI\_Gather} podemos juntar pedaços de dados distribuídos em um único processo. Analogamente, a rotina \href{https://www.open-mpi.org/doc/current/man3/MPI\_Allgather.3.php}{MPI\_Allgather} nos permite juntar os pedaços de dados distribuídos e ter uma cópia do todo em cada um dos processos. Sua sintaxe é a seguinte
\begin{verbatim}
int MPI_Allgather(
  const void *sendbuf, 
  int  sendcount,
  MPI_Datatype sendtype, 
  void *recvbuf, 
  int recvcount,
  MPI_Datatype recvtype, 
  MPI_Comm comm)
\end{verbatim}
  Note que esta rotina não contém o argumento \verb+root+, pois neste caso todos os processos receberam os dados juntados na variável \verb+recvbuf+!
\end{obs}

\subsection*{Exercícios}
\badge[yellow]{Em revisão}

\begin{exer}
  Faça um código MPI para computar a média aritmética simples de $n$ números randômicos em ponto flutuante.
\end{exer}

\begin{exer}
  Faça um código MPI para computar o produto interno de dois vetores de $n$ elementos randômicos em ponto flutuante.
\end{exer}

\begin{exer}
  Faça um código MPI para computar a norma $L^2$ de um vetor de $n$ elementos randômicos em ponto flutuante.
\end{exer}

\begin{exer}
  Faça um código MPI para computar
  \begin{equation}
    erf(x) = \frac{2}{\sqrt{\pi}}\int_0^x e^{-t^2}\,dt
  \end{equation}
  usando a \href{https://phkonzen.github.io/notas/MatematicaNumerica/cap_integr_sec_int_comp.html}{regra composta do ponto médio}.
\end{exer}

\begin{exer}
  Faça uma implementação MPI do método de Jacobi para computar a solução de um sistema $Ax=b$ $n\times n$. Inicialize $A$ e $b$ com números randômicos em ponto flutuante.
\end{exer}


\section {Reduções} \label {cap_mpi_sec_redu}
\badge[yellow]{Em revisão}

Reduções são rotinas que reduzem um conjunto de dados em um conjunto menor de dados. Um exemplo de redução é a rotina de calcular o traço de uma matriz quadrada. Aqui, vamos apresentar algumas soluções MPI de rotinas de redução.

A rotina \href{https://www.open-mpi.org/doc/current/man3/MPI_Reduce.3.php}{MPI\_Reduce}, permite a redução de dados distribuídos em um processo. Sua sintaxe é a seguinte
\begin{verbatim}
int MPI_Reduce(
  const void *sendbuf, 
  void *recvbuf, 
  int count,
  MPI_Datatype datatype, 
  MPI_Op op, 
  int root,
  MPI_Comm comm)
\end{verbatim}
O argumento \verb+sendbuf+ aponta para o endereço de memória do dado a ser enviado por cada processo, enquando que o argumento \verb+recvbuf+ aponta para o endereço de memória onde o resultado da redução será alocada no processo \verb+root+. O argumento \verb+count+ é o número de dados enviados por cada processo e \verb+datatype+ é o tipo de dado a ser enviado (o qual deve ser igual ao tipo do resultado da redução). O argumento \verb+comm+ é o comunicador entre os processos envolvidos na redução. A operação de redução é definida pelo argumento \verb+op+ e pode ser um dos seguintes
\begin{verbatim}
    MPI_MAX             maximum
    MPI_MIN             minimum
    MPI_SUM             sum
    MPI_PROD            product
    MPI_LAND            logical and
    MPI_BAND            bit-wise and
    MPI_LOR             logical or
    MPI_BOR             bit-wise or
    MPI_LXOR            logical xor
    MPI_BXOR            bit-wise xor
    MPI_MAXLOC          max value and location
    MPI_MINLOC          min value and location
\end{verbatim}

\begin{ex}
  No seguinte código \verb+reduce.cc+, cada processo aloca um número randômico na variável \verb+x+. Em seguida, o processo $0$ recebe o máximo entre os números alocados em todos os processos (inclusive no processo 0).
  
  \lstinputlisting[title={Código: reduce.cc}]{cap_mpi/dados/cc_Reduce/reduce.cc}

  Segue um teste de rodagem:
\begin{verbatim}
$ mpic++ reduce.cc
$ mpirun -np 3 a.out                                                           
0: 0.417425
1: 0.776647
2: 0.633021
Máx. entre os números = 0.776647
\end{verbatim}
  Verifique!
\end{ex}

No caso de uma {\it array} de dados, a operação de redução será feita para cada componente. Veja o próximo exemplo.

\begin{ex}
  No \verb+reduce2.cc+, cada processo aloca um vetor com dois números randômicos. Em seguida, o processo $0$ recebe o vetor resultante da soma dos vetores alocados em cada processo (inclusive no processo 0).

  \lstinputlisting[title={Código: reduce2.cc}]{cap_mpi/dados/cc_Reduce/reduce2.cc}

  Segue um teste de rodagem:
\begin{verbatim}
$ mpic++ reduce2.cc
$ mpirun -np 3 a.out 
0: 0.193702 0.035334
Vetor soma = 0.656458 0.843728
1: 0.051281 0.447279
2: 0.411475 0.361114
\end{verbatim}
  Verifique!
\end{ex}

\begin{obs}
  A rotina \href{https://www.open-mpi.org/doc/current/man3/MPI\_Allreduce.3.php}{MPI\_Allreduce} executa uma redução de dados e o resultado é alocada em todos os processos. Sua sintaxe é similar a rotina \href{https://www.open-mpi.org/doc/current/man3/MPI_Reduce.3.php}{MPI\_Reduce}, com exceção do argumento \verb+root+, o qual não é necessário nessa rotina. Verifique!
\end{obs}

\begin{obs}
  As rotinas \href{https://www.open-mpi.org/doc/current/man3/MPI\_Ireduce.3.php}{MPI\_Ireduce} e \href{https://www.open-mpi.org/doc/current/man3/MPI\_Iallreduce.3.php}{MPI\_Iallreduce} são versões assíncronas das rotinas \href{https://www.open-mpi.org/doc/current/man3/MPI_Reduce.3.php}{MPI\_Reduce} e \href{https://www.open-mpi.org/doc/current/man3/MPI\_Allreduce.3.php}{MPI\_Allreduce}, respectivamente.
\end{obs}

\subsection{Exercícios}
\badge[yellow]{Em revisão}

\begin{exer}
  Faça um código MPI em cada processo aloca um número randômico na variável \verb+x+. Então, o processo 0 recebe o mínimo entre os números alocados em cada processo, inclusive nele mesmo.
\end{exer}

\begin{exer}
  Faça um código MPI em cada processo aloca um número randômico na variável \verb+x+. Então, o processo 0 recebe o produtório entre os números alocados em cada processo, inclusive nele mesmo.
\end{exer}

\begin{exer}
  Faça um código MPI para computar a soma dos termos de um vetor de $n$ números randômicos, com $n\gg n_p$, sendo $n_p$ o número de processos. Use a rotina \href{https://www.open-mpi.org/doc/current/man3/MPI_Reduce.3.php}{MPI\_Reduce} e certifique-se de que cada processo aloque somente os dados necessários, otimizando o uso de memória computacional.
\end{exer}

\begin{exer}
  Faça um código MPI para computar a norma do máximo de um vetor de $n$ números randômicos, com $n\gg n_p$, sendo $n_p$ o número de processos. Use a rotina \href{https://www.open-mpi.org/doc/current/man3/MPI_Reduce.3.php}{MPI\_Reduce} e certifique-se de que cada processo aloque somente os dados necessários, otimizando o uso de memória computacional.
\end{exer}

\begin{exer}
  Faça um código MPI para computar a norma $L^2$ de um vetor de $n$ números randômicos, com $n\gg n_p$, sendo $n_p$ o número de processos. Use a rotina \href{https://www.open-mpi.org/doc/current/man3/MPI_Allreduce.3.php}{MPI\_Allreduce} de forma que cada processo contenha a norma computada ao final do código.
\end{exer}

\begin{exer}
  Faça um código MPI para computar
  \begin{equation}
    erf(x) = \frac{2}{\sqrt{\pi}}\int_0^x e^{-t^2}\,dt
  \end{equation}
  usando a \href{https://phkonzen.github.io/notas/MatematicaNumerica/cap_integr_sec_int_comp.html}{regra composta do ponto médio}. Use a rotina \href{https://www.open-mpi.org/doc/current/man3/MPI_Reduce.3.php}{MPI\_Reduce}.
\end{exer}

\section {Aplicação: Equação do calor}\label{cap_mpi_sec_calor}
\badge[yellow]{Em revisão}

Nesta seção, vamos construir um código MPI para a resolução da equação do calor pelo método das diferenças finitas. Como um caso teste, vamos considerar a equação do calor
\begin{equation}\label{eq:mpi_calor_eq}
  u_t - u_{xx} = 0,\quad 0<x<1, t>0,
\end{equation}
com condições de contorno
\begin{equation}\label{eq:mpi_calor_cc}
  u(0,t) = u(1,t) = 0, t>0,
\end{equation}
e condições iniciais
\begin{equation}\label{eq:mpi_calor_ci}
  u(x,0) = \sen(\pi x), 0\leq x\leq 1,
\end{equation}
onde, $u = u(t, x)$.

Seguindo o método de Rothe\footnote{Erich Hans Rothe, 1895 - 1988, matemático alemão. Fonte: \href{https://pt.wikipedia.org/wiki/Erich_Rothe}{Wikipédia}.}\footnote{Também chamado de método das linhas.}, vamos começar pela discretização no tempo. Para tanto, vamos usar a notação $t_m = m\cdot h_t$, $m=0,1,2,\dotsc,M$, onde $h_t$ é o passo no tempo e $M$ o número total de iterações no tempo. Usando o método de Euler\footnote{Leonhard Paul Euler, 1707 - 1783, matemático suíço. Fonte: \href{https://pt.wikipedia.org/wiki/Leonhard_Euler}{Wikipédia}.}, a equação \eqref{eq:mpi_calor_eq} é aproximada por
\begin{equation}\label{eq:mpi_color_eq_euler}
  u(t_{m+1}, x) \approx u(t_m, x) + h_t u_{xx}(t_m, x),\quad 0<x<1,
\end{equation}
onde $u(t_0, x) = u(0, x)$, dada pela condição inicial \eqref{eq:mpi_calor_ci}.

Agora, vamos denotar $x_i=ih_x$, $i=0,1,2,\dotsc,I$, onde $h_x=1/I$ é o tamanho da malha (passo espacial). Então, usando a aproximação por diferenças finitas centrais de ordem 2, obtemos
\begin{equation}
  u(t_{m+1}, x_i) \approx u(t_m, x_i) + h_t \left[\frac{u(t_m,x_{i-1}) - 2u(t_m, x_i) + u(t_m, x_{i+1})}{h_x^2}\right],
\end{equation}
para $i=1,2,\dotsc,I-1$, observando que, das condições de contorno \eqref{eq:mpi_calor_cc}, temos
\begin{equation}
  u(t_m, x_0) = u(t_m, x_I) = 0.
\end{equation}

Em síntese, denotando $u^m_i \approx u(t_m, x_i)$, temos que uma aproximação da solução do problema de calor acima pode ser calculada pela seguinte iteração
\begin{equation}
  u^{m+1}_i = u^m_i + \frac{h_t}{h_x^2}u^m_{i-1} - 2\frac{h_t}{h_x^2}u^m_{i} + \frac{h_t}{h_x^2}u^m_{i+1},
\end{equation}
com $u^0_i = \sen(\pi x_i)$ e $u^m_0=u^m_I = 0$.

Para construirmos um código MPI, vamos fazer a distribuição do processamento pela malha espacial entre os $n_p$ processos disponíveis. Mais precisamente, cada processo $p = 0, 1, 2, \dotsc, n_p-1$, computará a solução $u^m_i$ nos ponto de malha $i=i_p, i_p+1, \dotsc, f_p$, onde
\begin{gather}
  i_p = p\left\lfloor\frac{I}{n_p}\right\rfloor, \quad p=0,1,\dotsc,n_p-1,\\
  f_p = (p+1)\left\lfloor\frac{I}{n_p}\right\rfloor,\quad p=0,1,\dotsc,n_p-2,
\end{gather}
com $f_{n_p-1}=I$. Ainda, por simplicidade e economia de memória computacional, vamos remover o superíndice, denotando por $u$ a solução na iteração corrente e por $u^0$ a solução na iteração anterior.

Neste caso, a cada iteração $m=0,1,2,\ldots, M$, o processo $p=0$, irá computar
\begin{equation}
  u_j = u^0_j + \frac{h_t}{h_x^2}u^0_{j-1} - 2\frac{h_t}{h_x^2}u^0_{j} + \frac{h_t}{h_x^2}u^0_{j+1},
\end{equation}
para $j=1,\dotsc,f_0$, com as condições de contorno
\begin{gather}
  u^0_0 = 0\\
  u^0_{f_0+1} = u^0_{i_1+1}.
\end{gather}
Ou seja, este passo requer a comunicação entre o processo 0 e o processo 1.

Os processos $p=1,2,\dotsc,n_p-2$ irão computar
\begin{equation}
  u_j = u^0_j + \frac{h_t}{h_x^2}u^0_{j-1} - 2\frac{h_t}{h_x^2}u^0_{j} + \frac{h_t}{h_x^2}u^0_{j+1},
\end{equation}
para $j=i_p+1,\dotsc, f_p$, com as condições de contorno
\begin{gather}
  u^0_{i_p} = u^0_{f_{p-1}}\\
  u^0_{f_p+1} = u^0_{i_{p+1}+1}.
\end{gather}
Logo, este passo requer a comunicação entre os processos $p-1$, $p$ e $p+1$.

Ainda, o processo $p=n_p-1$ deve computar
\begin{equation}
  u_j = u^0_j + \frac{h_t}{h_x^2}u^0_{j-1} - 2\frac{h_t}{h_x^2}u^0_{j} + \frac{h_t}{h_x^2}u^0_{j+1},
\end{equation}
para $j=i_{n_p-1}+1,\dotsc, f_{n_p-1}-1$, com as condições de contorno
\begin{gather}
  u^0_{i_{n_p-1}} = u^0_{f_{n_p-2}}\\
  u^0_{f_{n_p-1}} = 0.
\end{gather}
O que requer a comunicação entre os processos $n_p-2$ e o $n_p-1$.

O código abaixo \verb+calor.cc+ é uma implementação MPI deste algoritmo. Cada processo aloca e computa a solução em apenas um pedaço da malha, conforme descrito acima. As comunicações entre os processos ocorrem apenas nas linhas 60-80. Verifique!

\lstinputlisting[title={Código: calor.cc}]{cap_mpi/dados/cc_calor/calor.cc}

No código acima, as comunicações entre os processos foram implementadas de forma cíclica. A primeira a ocorrer é o envio de $u^0_{f_p}$ do processo zero (linhas 61-62) e o recebimento de $u^0_{i_p}$ pelo processo 1 (linhas 67-69). Enquanto essa comunicação não for completada, todos os processos estarão aguardando, sincronizados nas linhas 67-69 e 79-81. Ocorrida a comunicação entre os processos 0 e 1, ocorrerá a comunicação entre o 1 e o 2, entre o 2 e o 3 e assim por diante, de forma cíclica (linhas 67-71 e 79-81).

As últimas comunicações também ocorrem de forma cíclica, começando pelo envio de $u^0_{i_p+1}$ pelo processo $n_p-1$ (linhas 81-83) e o recebimento de $u^0_{f_p+1}$ pelo processo $n_p-2$ (linhas 73-75). Em sequência, ocorrem as comunicações entre o processo $n_p-2$ e o processo $n_p-3$, entre o $n_p-3$ e o $n_p-4$, até o término das comunicações entre o processo 1 (linhas 76-77) e o processo 0 (linhas 63-65).

\subsection{Exercícios}
\badge[yellow]{Em revisão}

\begin{exer}\label{exer:mpi_calor_sa}
  O problema \eqref{eq:mpi_calor_eq}-\eqref{eq:mpi_calor_ci} tem solução analítica
  \begin{equation}
    u(t,x) = e^{-\pi^2 t}\sen(\pi x). 
  \end{equation}
  Modifique o código \verb+calor.cc+ acima, de forma que a acada iteração $m$ no tempo, seja impresso na tela o valor de $u^m(0.5)$ computado e o valor da solução analítica $u(t_m, 0.5)$. Faça refinamentos nas malhas temporal e espacial, até conseguir uma aproximação com três dígitos significativos corretos.
\end{exer}

\begin{exer}
  Modifique o código \verb+calor.cc+ acima, de forma que a cada iteração $m$ no tempo, seja impresso na tela o valor do funcional
  \begin{equation}
    q(t) = \int_0^1 u(t,x)\,dx.
  \end{equation}
  Valide os resultados por comparação com a solução analítica (veja o Exercício \eqref{exer:mpi_calor_sa}).
\end{exer}

\begin{exer}
  Modifique o código \verb+calor.cc+ acima, de forma que as comunicações iniciem-se entre os processos $n_p-1$ e $n_p-2$. Ou seja, que a primeira comunicação seja o envio de $u^0_{i_p+1}$ pelo processo $n_p-1$ e o recebimento de $u^0_{f_p+1}$ pelo processo $n_p-2$. E que, de forma cíclica, as demais comunicações ocorram. Poderia haver alguma vantagem em fazer esta modificação?
\end{exer}

\begin{exer}
  Modifique o código \verb+calor.cc+ acima, de forma que as comunicações sejam feitas de forma assíncrona, i.e. usando as rotinas \href{https://www.open-mpi.org/doc/v1.8/man3/MPI\_Isend.3.php}{MPI\_Isend} e \href{https://www.open-mpi.org/doc/v1.8/man3/MPI\_Irecv.3.php}{MPI\_Irecv}. Há vantagens em se fazer esta modificação?
\end{exer}

\begin{exer}
  Faça um código MPI para computar uma aproximação da solução de
  \begin{equation}
    u_t - u_{xx} = 0,\quad 0<x<1, t>0,
  \end{equation}
  com condições de contorno periódicas
  \begin{equation}
    u(0,t) = u(1,t), t>0,
  \end{equation}
  e condições iniciais
  \begin{equation}
    u(x,0) = \sen(\pi x), 0\leq x\leq 1,
  \end{equation}
  onde, $u = u(t, x)$.  
\end{exer}



